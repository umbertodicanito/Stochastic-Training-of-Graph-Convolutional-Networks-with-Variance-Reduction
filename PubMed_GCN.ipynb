{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "PubMed_GCN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umbertodicanito/Stochastic-Training-of-Graph-Convolutional-Networks-with-Variance-Reduction/blob/master/PubMed_GCN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inaEyeyeN3mP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4C3py11yrk_",
        "colab_type": "text"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh4yOJOUN5kV",
        "colab_type": "code",
        "outputId": "8bdc4ff2-be63-411f-b7ec-5b400b3ba4be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "!pip install dgl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dgl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/ba/d15ce7fb56958f21d4e9815f96344d92c4982f3fb933a0e987e78cb787e5/dgl-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.4MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4MB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.3.3)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.6/dist-packages (from dgl) (2.4)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from dgl) (1.17.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.1->dgl) (4.4.1)\n",
            "Installing collected packages: dgl\n",
            "Successfully installed dgl-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVHZxjokN3mX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import dgl\n",
        "import dgl.function as fn\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from dgl import DGLGraph\n",
        "from google.colab import files\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "gcn_msg = fn.copy_src(src='h', out='m')\n",
        "gcn_reduce = fn.sum(msg='m', out='h')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbLw1KjvIXxt",
        "colab_type": "text"
      },
      "source": [
        "#Building matrices for algorithm 1 execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Us006QFON3ml",
        "colab_type": "code",
        "outputId": "9ea48167-fe36-4463-ce54-706970f5140b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from dgl.data import citation_graph as citegrh\n",
        "import networkx as nx\n",
        "def load_pubmed_data():\n",
        "    data = citegrh.load_pubmed()\n",
        "    features = th.FloatTensor(data.features)\n",
        "    labels = th.LongTensor(data.labels)\n",
        "    mask = th.ByteTensor(data.train_mask)\n",
        "    g = data.graph\n",
        "    print(g)\n",
        "    # add self loop\n",
        "    g.remove_edges_from(nx.selfloop_edges(g))\n",
        "    g = DGLGraph(g)\n",
        "    g.add_edges(g.nodes(), g.nodes())\n",
        "    return g, features, labels, mask\n",
        "    \n",
        "#get data\n",
        "g, features, labels, mask = load_pubmed_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading /root/.dgl/pubmed.zip from https://s3.us-east-2.amazonaws.com/dgl.ai/dataset/pubmed.zip...\n",
            "Extracting file to /root/.dgl/pubmed\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv2IXZxXbUuy",
        "colab_type": "text"
      },
      "source": [
        "`deg_matrix` is the diagonal degree matrix.\n",
        "This means that the degree of a node `'v'` with value `'d'`, is stored at `D[u][u]=d`, whereas all the other values of the matrix (except the diagonal) are equal to 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6-kv5AzIcsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createMatrixP():\n",
        "  import math\n",
        "  deg_matrix = th.tensor(np.zeros((19717,19717)))\n",
        "  print(\"Constructing the degree matrix...\")\n",
        "  for i in range(19717):\n",
        "    d = len(g.adjacency_matrix()[i]._indices()[0])\n",
        "    deg_matrix[i][i] = math.pow(d,-0.5)\n",
        "\n",
        "  print(\"Computing matrix P...\")\n",
        "  p = th.mm(th.mm(deg_matrix.float(),g.adjacency_matrix().to_dense().float()),deg_matrix.float())\n",
        "  print(\"Matrix P has shape of:\")\n",
        "  print(p.shape)\n",
        "  return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THUMmsrHbPnT",
        "colab_type": "text"
      },
      "source": [
        "This is the propagation matrix P"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RJv6E9_az-g",
        "colab_type": "code",
        "outputId": "03b3ca37-2e15-4aa0-d8e9-5c642aabb364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "P = createMatrixP()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Constructing the degree matrix...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/dgl/base.py:25: UserWarning: Currently adjacency_matrix() returns a matrix with destination as rows by default.  In 0.5 the result will have source as rows (i.e. transpose=True)\n",
            "  warnings.warn(msg, warn_type)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Computing matrix P...\n",
            "Matrix P has shape of:\n",
            "torch.Size([19717, 19717])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wjPK7kubzpg",
        "colab_type": "text"
      },
      "source": [
        "The object `g` allows us to get more easily the neighboors nodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ-KmVf99CMB",
        "colab_type": "code",
        "outputId": "38d391a8-8515-42f0-879f-22c429c6ca87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "r = random.randint(0,19716)\n",
        "print(\"Node \" + str(r))\n",
        "g.adjacency_matrix()[r]._indices()[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Node 17222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 7712, 15025, 16747, 17222])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8fm-73C-T-D",
        "colab_type": "code",
        "outputId": "ccffcd90-de7c-4c88-8dd3-440db8a7a546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "a = list(g.adjacency_matrix()[r]._indices()[0].numpy())\n",
        "a.remove(r)\n",
        "print(type(a))\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "[7712, 15025, 16747]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNOMUjH6b_Px",
        "colab_type": "text"
      },
      "source": [
        "This is a simple method used to transforma a list into a torch.Tensor object corresponding to a mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TETrYuIvCrij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createMask(array):\n",
        "  a = []\n",
        "  for i in range(19717):\n",
        "    if i in array:\n",
        "      a.append(1)\n",
        "    else:\n",
        "      a.append(0)\n",
        "  m = th.ByteTensor(a)\n",
        "  return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z8ub6L6I1Wu",
        "colab_type": "code",
        "outputId": "71d4aa24-e540-468e-a60f-6c6620930ae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "plain_deg_matrix = np.zeros(19717)\n",
        "for i in range(19717):\n",
        "  d = len(g.adjacency_matrix()[i]._indices()[0])\n",
        "  plain_deg_matrix[i] = d"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/dgl/base.py:25: UserWarning: Currently adjacency_matrix() returns a matrix with destination as rows by default.  In 0.5 the result will have source as rows (i.e. transpose=True)\n",
            "  warnings.warn(msg, warn_type)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nJTtgo-w2MU",
        "colab_type": "text"
      },
      "source": [
        "#Implementing algorithm 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taLcl787s7zF",
        "colab_type": "text"
      },
      "source": [
        "The following algorithm is used in order to retrive the receptive fields of each layer and the propagation matrices for each layer, just for the selected minibath of nodes.\n",
        "\n",
        "The algorithm is used in algorithm for training with CV and CVD approaches.\n",
        "\n",
        "Pseudo-code:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "r_L = V_B\n",
        "for layer l = L - 1 to 0 do\n",
        "  r_l = 0\n",
        "  P'_l = 0\n",
        "  for each node u in r_l+1 do\n",
        "    r_l = r_l + {u}\n",
        "    P'_uu^l = P'_uu^l + P_uu*n(u)/D_l\n",
        "    for D_l - 1 random neighbors v in n(u) do\n",
        "      r_l = r_l + {v}\n",
        "      P'_uv^l = P'_uv^l + P_uv*n(u)/D_l\n",
        "    end for\n",
        "  end for\n",
        "end for\n",
        "```\n",
        "where\n",
        "\n",
        "\n",
        "*   r_L: the receptive field of layer L\n",
        "*   V_B: the minibatch set (a subset of nodes)\n",
        "*   P'_l: propagation matrix of the layer l\n",
        "*   P'_uv^l = P_uv^l * n(u)/D_l, if v is in n'(u)_l, otherwise 0\n",
        "*   n(u): neighbors of u\n",
        "*   n'(u): random subset of n(u)\n",
        "*   D_l: neighbors for each node at layer l\n",
        "\n",
        "Notice that, since we do not use MINIbatches, n(u)/D_l is always equal to 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-xBNuIIBfhdd",
        "colab": {}
      },
      "source": [
        "import time\n",
        "#implementation of the above algorithm#\n",
        "\n",
        "#minibatch = th.ByteTensor(...) of 0,1 boolean (it is a mask)\n",
        "\n",
        "#returning: the receptive fields and the propagation matrices\n",
        "def algOne(minibatch):\n",
        "  t0 = time.time()\n",
        "  rL = minibatch\n",
        "  l = 2\n",
        "  n = 2\n",
        "\n",
        "  receptiveField = dict()\n",
        "  propagationMatrix = dict()\n",
        "\n",
        "  receptiveField[l] = rL\n",
        "\n",
        "  diffZero = dict()\n",
        "  values = dict()\n",
        "\n",
        "  #first for-loop\n",
        "  while l > 0:\n",
        "    #init\n",
        "    field = []\n",
        "    #second for-loop\n",
        "    k = 0\n",
        "    for nodeValue in receptiveField[l]:\n",
        "      if nodeValue == 1:\n",
        "        node = k\n",
        "\n",
        "        if node not in field:\n",
        "          field.append(node)\n",
        "\n",
        "        toAdd = P[node][node] * plain_deg_matrix[node] / n\n",
        "\n",
        "        if node not in diffZero:\n",
        "          diffZero[node] = []\n",
        "          values[node] = dict()\n",
        "        if node not in diffZero[node]:\n",
        "          diffZero[node].append(node)\n",
        "          values[node][node] = 0\n",
        "        \n",
        "        values[node][node] = values[node][node] + toAdd\n",
        "\n",
        "        #collecting n random neighbor\n",
        "        subset = []\n",
        "        a = list(g.adjacency_matrix()[node]._indices()[0].numpy())\n",
        "        a.remove(node)\n",
        "        if len(a) <= n:\n",
        "          subset = a\n",
        "        else:\n",
        "          subset = random.sample(a, k=n)\n",
        "\n",
        "        #last for-loop\n",
        "        for neighbor in subset:\n",
        "          if neighbor not in field:\n",
        "            field.append(neighbor)\n",
        "\n",
        "          toAddForNeighbor = P[node][neighbor] * plain_deg_matrix[node]/ n\n",
        "\n",
        "          if neighbor not in diffZero[node]:\n",
        "            diffZero[node].append(neighbor)\n",
        "            values[node][neighbor] = 0\n",
        "          values[node][neighbor] = values[node][neighbor] + toAddForNeighbor\n",
        "      k = k + 1\n",
        "      \n",
        "    #updating level\n",
        "    l = l - 1\n",
        "\n",
        "    #convert field (array of nodes) to a mask\n",
        "    receptiveField[l] = createMask(field).detach()\n",
        "\n",
        "    #creating sparse matrix\n",
        "    X = []\n",
        "    Y = []\n",
        "    V = []\n",
        "    for x in diffZero:\n",
        "      for y in diffZero[x]:\n",
        "        X.append(x)\n",
        "        Y.append(y)\n",
        "        V.append(values[x][y])\n",
        "      \n",
        "    index = th.LongTensor([X,Y])\n",
        "    valuesOfProp = th.FloatTensor(V)\n",
        "    propagationMatrix[l] = th.sparse.FloatTensor(index, valuesOfProp, th.Size([19717,19717]))\n",
        "  \n",
        "  #print(time.time() - t0)\n",
        "  return [receptiveField, propagationMatrix]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLyJPLrTjKky",
        "colab_type": "text"
      },
      "source": [
        "Testing algorithm one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDs2Q7PxjJ_v",
        "colab_type": "code",
        "outputId": "adbdcbcb-179a-49bc-d31e-eff53f078581",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "import time\n",
        "\n",
        "t0 = time.time()\n",
        "res = algOne(mask)\n",
        "print(time.time()-t0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/dgl/base.py:25: UserWarning: Currently adjacency_matrix() returns a matrix with destination as rows by default.  In 0.5 the result will have source as rows (i.e. transpose=True)\n",
            "  warnings.warn(msg, warn_type)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1.8744902610778809\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1GVRCyIN3mW",
        "colab_type": "text"
      },
      "source": [
        "#Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrKmDjqXcIuC",
        "colab_type": "text"
      },
      "source": [
        "Some commond parameters used to train the NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45EUaxAcD04F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_of_training_cycles = 10\n",
        "n_of_epochs = 65"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45QJRof3cNIN",
        "colab_type": "text"
      },
      "source": [
        "This methods creates some random masks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVcFLcl8V8vY",
        "colab_type": "code",
        "outputId": "0c188afe-0682-406e-c970-01edb6a447b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#random mask\n",
        "masks = []\n",
        "n_masks = 10\n",
        "size_masks = 100\n",
        "a = range(19717)\n",
        "for i in range(n_masks):\n",
        "  mask = np.zeros(19717)\n",
        "  sub = random.sample(a,k=size_masks)\n",
        "  for s in sub:\n",
        "    mask[s] = 1\n",
        "  m = th.ByteTensor(mask)\n",
        "  masks.append(m)\n",
        "\n",
        "print(\"Generated \" + str(n_masks) + \" random masks with size \" + str(size_masks))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generated 10 random masks with size 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgeKlv_CN3ma",
        "colab_type": "text"
      },
      "source": [
        "We then define the node UDF for ``apply_nodes``, which is a fully-connected layer:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcTM-w1gN3mb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NodeApplyModule(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, activation):\n",
        "        super(NodeApplyModule, self).__init__()\n",
        "        self.linear = nn.Linear(in_feats, out_feats)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, node):\n",
        "        z = self.linear(node.data['h'])\n",
        "        h = self.activation(z)\n",
        "        return {'h' : h}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD1DNkN8N3md",
        "colab_type": "text"
      },
      "source": [
        "We then proceed to define the GCN module. A GCN layer essentially performs\n",
        "message passing on all the nodes then applies the `NodeApplyModule`. Note\n",
        "that we omitted the dropout in the paper for simplicity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQy_eUuWN3me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GCN(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, activation):\n",
        "        super(GCN, self).__init__()\n",
        "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
        "\n",
        "    def forward(self, g, feature):\n",
        "        g.ndata['h'] = feature\n",
        "        g.update_all(gcn_msg, gcn_reduce)\n",
        "        g.apply_nodes(func=self.apply_mod)\n",
        "        return g.ndata.pop('h')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEMMI4CXN3mh",
        "colab_type": "text"
      },
      "source": [
        "The forward function is essentially the same as any other commonly seen NNs\n",
        "model in PyTorch.  We can initialize GCN like any ``nn.Module``. For example,\n",
        "let's define a simple neural network consisting of two GCN layers. Suppose we\n",
        "are training the classifier for the PubMed dataset (the input feature size is\n",
        "1433 and the number of classes is 7).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVVOgunmN3mh",
        "colab_type": "code",
        "outputId": "788ce14d-126e-40e9-fb4a-e9b14000157b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.gcn1 = GCN(500, 16, F.relu)\n",
        "        self.gcn2 = GCN(16, 3, F.relu)\n",
        "\n",
        "    def forward(self, g, features):\n",
        "        x = self.gcn1(g, features)\n",
        "        x = self.gcn2(g, x)\n",
        "        return x\n",
        "        \n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (gcn1): GCN(\n",
            "    (apply_mod): NodeApplyModule(\n",
            "      (linear): Linear(in_features=500, out_features=16, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (gcn2): GCN(\n",
            "    (apply_mod): NodeApplyModule(\n",
            "      (linear): Linear(in_features=16, out_features=3, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V5DJfryN3mk",
        "colab_type": "text"
      },
      "source": [
        "We load the PubMed dataset using DGL's built-in data module.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFzGHVDwN3mn",
        "colab_type": "text"
      },
      "source": [
        "We then train the network as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9z-t8-jmRLM",
        "colab_type": "text"
      },
      "source": [
        "This is the implementation of a standard GCN with 2 layers (+ 1 input layer). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0RwjiSi5Fz5",
        "colab_type": "text"
      },
      "source": [
        "TEST --------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJSEARJJ9MWr",
        "colab_type": "text"
      },
      "source": [
        "In order to get the most effective learning rate for the CiteSeer set, running these cells multiple times will show comparison of performance between 4 different learning rates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53dK1Oeb6gwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrAccTest = dict()\n",
        "lrLossTest = dict()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sAwautQX5zYG",
        "colab": {}
      },
      "source": [
        "def updatePoints(index,accCS,lossCS, acc, loss):\n",
        "  accPointsTest = [0]*30\n",
        "  lossPointsTest = [0]*30\n",
        "\n",
        "  for d in range(3):\n",
        "    for i in range(30):\n",
        "      accPointsTest[i] = accPointsTest[i] + accCS[d][i]\n",
        "      lossPointsTest[i] = lossPointsTest[i] + lossCS[d][i]\n",
        "\n",
        "  for i in range(30):\n",
        "    accPointsTest[i] = accPointsTest[i]/3\n",
        "    lossPointsTest[i] = lossPointsTest[i]/3\n",
        "  \n",
        "  acc[index] = accPointsTest\n",
        "  loss[index] = lossPointsTest\n",
        "  return [acc,loss]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlaqREoy5FRb",
        "colab_type": "code",
        "outputId": "b2255d8e-b059-4bff-f00e-e39f94a4c409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "import collections\n",
        "import numpy as np\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#init counters\n",
        "lrs = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "for actualLR in lrs:\n",
        "  testAccCS = []\n",
        "  testLossCS = []\n",
        "  for t in range(3):\n",
        "    print(\"Starting new training\")\n",
        "    #get data from DGL\n",
        "    net = Net()\n",
        "    g, features, labels, mask = load_pubmed_data()\n",
        "    print(g)\n",
        "\n",
        "    #point to show on graph\n",
        "    pointsPubMed=dict()\n",
        "    pointsLossPubMed=dict()\n",
        "\n",
        "    #the number of masks to get when training per epoch\n",
        "    n_masks_to_try = 4\n",
        "    #initializing the optimizer (optimizer takes care of optimize the learining rate during training)\n",
        "    optimizer = th.optim.Adam(net.parameters(), lr=actualLR)\n",
        "    optimizer.state = collections.defaultdict(dict)\n",
        "\n",
        "    #dur is just an array to store the duration in order to show them later\n",
        "    dur = []\n",
        "\n",
        "    for epoch in range(30):\n",
        "        t0 = time.time()\n",
        "\n",
        "        #getting only some masks (4)\n",
        "        masksToTry = random.sample(masks,n_masks_to_try)\n",
        "\n",
        "        for m in masksToTry:\n",
        "            #calling 'net(...)' it asks to the GCN to compute the forward    \n",
        "\n",
        "            logits = net(g, features)\n",
        "            logp = F.log_softmax(logits, 1)\n",
        "\n",
        "            #compute loss like the negative log likelihood loss\n",
        "            loss = F.nll_loss(logp[m], labels[m])\n",
        "            \n",
        "            #Since the backward() function accumulates gradients, and you \n",
        "            #don’t want to mix up gradients between minibatch, you have \n",
        "            #to zero them out at the start of a new minibatch. This is \n",
        "            #exactly like how a general (additive) accumulator variable is \n",
        "            #initialized to 0 in code.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            #update network weights by loss\n",
        "            loss.backward()\n",
        "\n",
        "            #update optimizer's values after backward\n",
        "            optimizer.step()\n",
        "\n",
        "            #computing accuracy\n",
        "            i = 0\n",
        "            matched = 0\n",
        "            while i < 19717:\n",
        "              if m[i] == 0:\n",
        "                #getting index of the maximum\n",
        "                j = 0\n",
        "                max = None\n",
        "                jMax = 0\n",
        "                for a in logp[i]:\n",
        "                  if max==None:\n",
        "                    max = a.item()\n",
        "                    jMax = j\n",
        "                  elif max < a.item():\n",
        "                    max = a.item()\n",
        "                    jMax = j\n",
        "                  j = j + 1\n",
        "                if jMax == labels[i]:\n",
        "                  matched = matched + 1\n",
        "              i = i + 1\n",
        "            acc = matched/(19717-size_masks)*100\n",
        "\n",
        "            if epoch not in pointsPubMed:\n",
        "              pointsPubMed[epoch] = 0\n",
        "              pointsLossPubMed[epoch] = 0\n",
        "            pointsPubMed[epoch] = pointsPubMed[epoch] + acc\n",
        "            pointsLossPubMed[epoch] = pointsLossPubMed[epoch] + loss.item()\n",
        "            \n",
        "        dur.append(time.time() - t0)\n",
        "        \n",
        "        #computing the average of the accuracy and the loss\n",
        "        pointsPubMed[epoch] = pointsPubMed[epoch]/n_masks_to_try\n",
        "        pointsLossPubMed[epoch] = pointsLossPubMed[epoch]/n_masks_to_try\n",
        "\n",
        "        print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f} | Accuracy: {:.6f} %\".format(\n",
        "                epoch, loss.item(), np.mean(dur), pointsPubMed[epoch]))\n",
        "    #storing results    \n",
        "    testAccCS.append(pointsPubMed)\n",
        "    testLossCS.append(pointsLossPubMed)\n",
        "    print(\"Results stored.\")\n",
        "  print(\"New learning rate...\")\n",
        "  r = updatePoints(actualLR,testAccCS,testLossCS, lrAccTest, lrLossTest)\n",
        "  lrAccTest = r[0]\n",
        "  lrLossTest = r[1]\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0686 | Time(s) 5.5194 | Accuracy: 39.114034 %\n",
            "Epoch 00001 | Loss 1.0986 | Time(s) 5.5088 | Accuracy: 21.524698 %\n",
            "Epoch 00002 | Loss 1.0986 | Time(s) 5.4561 | Accuracy: 20.809757 %\n",
            "Epoch 00003 | Loss 1.0986 | Time(s) 5.4125 | Accuracy: 20.817403 %\n",
            "Epoch 00004 | Loss 1.0986 | Time(s) 5.3876 | Accuracy: 20.814854 %\n",
            "Epoch 00005 | Loss 1.0986 | Time(s) 5.3926 | Accuracy: 20.813580 %\n",
            "Epoch 00006 | Loss 1.0986 | Time(s) 5.3820 | Accuracy: 20.813580 %\n",
            "Epoch 00007 | Loss 1.0986 | Time(s) 5.3848 | Accuracy: 20.816129 %\n",
            "Epoch 00008 | Loss 1.0986 | Time(s) 5.3796 | Accuracy: 20.812306 %\n",
            "Epoch 00009 | Loss 1.0986 | Time(s) 5.3833 | Accuracy: 20.814854 %\n",
            "Epoch 00010 | Loss 1.0986 | Time(s) 5.3899 | Accuracy: 20.814854 %\n",
            "Epoch 00011 | Loss 1.0986 | Time(s) 5.3938 | Accuracy: 20.804659 %\n",
            "Epoch 00012 | Loss 1.0986 | Time(s) 5.3942 | Accuracy: 20.807208 %\n",
            "Epoch 00013 | Loss 1.0986 | Time(s) 5.3995 | Accuracy: 20.816129 %\n",
            "Epoch 00014 | Loss 1.0986 | Time(s) 5.3988 | Accuracy: 20.822501 %\n",
            "Epoch 00015 | Loss 1.0986 | Time(s) 5.3917 | Accuracy: 20.816129 %\n",
            "Epoch 00016 | Loss 1.0986 | Time(s) 5.3914 | Accuracy: 20.811031 %\n",
            "Epoch 00017 | Loss 1.0986 | Time(s) 5.3910 | Accuracy: 20.809757 %\n",
            "Epoch 00018 | Loss 1.0986 | Time(s) 5.3971 | Accuracy: 20.812306 %\n",
            "Epoch 00019 | Loss 1.0986 | Time(s) 5.3965 | Accuracy: 20.809757 %\n",
            "Epoch 00020 | Loss 1.0986 | Time(s) 5.3980 | Accuracy: 20.809757 %\n",
            "Epoch 00021 | Loss 1.0986 | Time(s) 5.4008 | Accuracy: 20.814854 %\n",
            "Epoch 00022 | Loss 1.0986 | Time(s) 5.4031 | Accuracy: 20.809757 %\n",
            "Epoch 00023 | Loss 1.0986 | Time(s) 5.3995 | Accuracy: 20.818678 %\n",
            "Epoch 00024 | Loss 1.0986 | Time(s) 5.4037 | Accuracy: 20.814854 %\n",
            "Epoch 00025 | Loss 1.0986 | Time(s) 5.4025 | Accuracy: 20.814854 %\n",
            "Epoch 00026 | Loss 1.0986 | Time(s) 5.4029 | Accuracy: 20.813580 %\n",
            "Epoch 00027 | Loss 1.0986 | Time(s) 5.4064 | Accuracy: 20.803385 %\n",
            "Epoch 00028 | Loss 1.0986 | Time(s) 5.4100 | Accuracy: 20.823775 %\n",
            "Epoch 00029 | Loss 1.0986 | Time(s) 5.4147 | Accuracy: 20.811031 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0986 | Time(s) 5.4872 | Accuracy: 28.113371 %\n",
            "Epoch 00001 | Loss 1.0986 | Time(s) 5.5377 | Accuracy: 20.812306 %\n",
            "Epoch 00002 | Loss 1.0986 | Time(s) 5.5730 | Accuracy: 20.817403 %\n",
            "Epoch 00003 | Loss 1.0986 | Time(s) 5.5540 | Accuracy: 20.818678 %\n",
            "Epoch 00004 | Loss 1.0986 | Time(s) 5.5405 | Accuracy: 20.812306 %\n",
            "Epoch 00005 | Loss 1.0986 | Time(s) 5.5227 | Accuracy: 20.816129 %\n",
            "Epoch 00006 | Loss 1.0986 | Time(s) 5.4844 | Accuracy: 20.809757 %\n",
            "Epoch 00007 | Loss 1.0986 | Time(s) 5.4686 | Accuracy: 20.803385 %\n",
            "Epoch 00008 | Loss 1.0986 | Time(s) 5.4636 | Accuracy: 20.813580 %\n",
            "Epoch 00009 | Loss 1.0986 | Time(s) 5.4604 | Accuracy: 20.809757 %\n",
            "Epoch 00010 | Loss 1.0986 | Time(s) 5.4512 | Accuracy: 20.813580 %\n",
            "Epoch 00011 | Loss 1.0986 | Time(s) 5.4384 | Accuracy: 20.816129 %\n",
            "Epoch 00012 | Loss 1.0986 | Time(s) 5.4310 | Accuracy: 20.807208 %\n",
            "Epoch 00013 | Loss 1.0986 | Time(s) 5.4274 | Accuracy: 20.817403 %\n",
            "Epoch 00014 | Loss 1.0986 | Time(s) 5.4211 | Accuracy: 20.804659 %\n",
            "Epoch 00015 | Loss 1.0986 | Time(s) 5.4286 | Accuracy: 20.812306 %\n",
            "Epoch 00016 | Loss 1.0986 | Time(s) 5.4269 | Accuracy: 20.809757 %\n",
            "Epoch 00017 | Loss 1.0986 | Time(s) 5.4218 | Accuracy: 20.816129 %\n",
            "Epoch 00018 | Loss 1.0986 | Time(s) 5.4267 | Accuracy: 20.807208 %\n",
            "Epoch 00019 | Loss 1.0986 | Time(s) 5.4218 | Accuracy: 20.813580 %\n",
            "Epoch 00020 | Loss 1.0986 | Time(s) 5.4182 | Accuracy: 20.809757 %\n",
            "Epoch 00021 | Loss 1.0986 | Time(s) 5.4149 | Accuracy: 20.811031 %\n",
            "Epoch 00022 | Loss 1.0986 | Time(s) 5.4096 | Accuracy: 20.803385 %\n",
            "Epoch 00023 | Loss 1.0986 | Time(s) 5.4029 | Accuracy: 20.807208 %\n",
            "Epoch 00024 | Loss 1.0986 | Time(s) 5.4025 | Accuracy: 20.816129 %\n",
            "Epoch 00025 | Loss 1.0986 | Time(s) 5.4062 | Accuracy: 20.809757 %\n",
            "Epoch 00026 | Loss 1.0986 | Time(s) 5.4100 | Accuracy: 20.814854 %\n",
            "Epoch 00027 | Loss 1.0986 | Time(s) 5.4081 | Accuracy: 20.811031 %\n",
            "Epoch 00028 | Loss 1.0986 | Time(s) 5.4042 | Accuracy: 20.811031 %\n",
            "Epoch 00029 | Loss 1.0986 | Time(s) 5.4012 | Accuracy: 20.818678 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.2973 | Time(s) 5.4964 | Accuracy: 41.749503 %\n",
            "Epoch 00001 | Loss 1.0986 | Time(s) 5.5203 | Accuracy: 27.804965 %\n",
            "Epoch 00002 | Loss 1.0986 | Time(s) 5.5045 | Accuracy: 20.813580 %\n",
            "Epoch 00003 | Loss 1.0986 | Time(s) 5.4616 | Accuracy: 20.814854 %\n",
            "Epoch 00004 | Loss 1.0986 | Time(s) 5.4385 | Accuracy: 20.819952 %\n",
            "Epoch 00005 | Loss 1.0986 | Time(s) 5.4270 | Accuracy: 20.816129 %\n",
            "Epoch 00006 | Loss 1.0986 | Time(s) 5.4189 | Accuracy: 20.804659 %\n",
            "Epoch 00007 | Loss 1.0986 | Time(s) 5.4108 | Accuracy: 20.809757 %\n",
            "Epoch 00008 | Loss 1.0986 | Time(s) 5.4117 | Accuracy: 20.808482 %\n",
            "Epoch 00009 | Loss 1.0986 | Time(s) 5.4056 | Accuracy: 20.805934 %\n",
            "Epoch 00010 | Loss 1.0986 | Time(s) 5.3998 | Accuracy: 20.812306 %\n",
            "Epoch 00011 | Loss 1.0986 | Time(s) 5.4016 | Accuracy: 20.817403 %\n",
            "Epoch 00012 | Loss 1.0986 | Time(s) 5.4054 | Accuracy: 20.814854 %\n",
            "Epoch 00013 | Loss 1.0986 | Time(s) 5.4034 | Accuracy: 20.814854 %\n",
            "Epoch 00014 | Loss 1.0986 | Time(s) 5.3946 | Accuracy: 20.807208 %\n",
            "Epoch 00015 | Loss 1.0986 | Time(s) 5.3933 | Accuracy: 20.809757 %\n",
            "Epoch 00016 | Loss 1.0986 | Time(s) 5.3938 | Accuracy: 20.805934 %\n",
            "Epoch 00017 | Loss 1.0986 | Time(s) 5.3938 | Accuracy: 20.813580 %\n",
            "Epoch 00018 | Loss 1.0986 | Time(s) 5.3952 | Accuracy: 20.809757 %\n",
            "Epoch 00019 | Loss 1.0986 | Time(s) 5.4014 | Accuracy: 20.809757 %\n",
            "Epoch 00020 | Loss 1.0986 | Time(s) 5.3968 | Accuracy: 20.822501 %\n",
            "Epoch 00021 | Loss 1.0986 | Time(s) 5.3893 | Accuracy: 20.817403 %\n",
            "Epoch 00022 | Loss 1.0986 | Time(s) 5.3936 | Accuracy: 20.818678 %\n",
            "Epoch 00023 | Loss 1.0986 | Time(s) 5.3954 | Accuracy: 20.818678 %\n",
            "Epoch 00024 | Loss 1.0986 | Time(s) 5.3930 | Accuracy: 20.814854 %\n",
            "Epoch 00025 | Loss 1.0986 | Time(s) 5.3880 | Accuracy: 20.809757 %\n",
            "Epoch 00026 | Loss 1.0986 | Time(s) 5.3833 | Accuracy: 20.813580 %\n",
            "Epoch 00027 | Loss 1.0986 | Time(s) 5.3844 | Accuracy: 20.816129 %\n",
            "Epoch 00028 | Loss 1.0986 | Time(s) 5.3849 | Accuracy: 20.812306 %\n",
            "Epoch 00029 | Loss 1.0986 | Time(s) 5.3867 | Accuracy: 20.807208 %\n",
            "Results stored.\n",
            "New learning rate...\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 0.9784 | Time(s) 5.5304 | Accuracy: 49.411225 %\n",
            "Epoch 00001 | Loss 0.8644 | Time(s) 5.4504 | Accuracy: 61.702860 %\n",
            "Epoch 00002 | Loss 0.8260 | Time(s) 5.4415 | Accuracy: 71.404904 %\n",
            "Epoch 00003 | Loss 0.8701 | Time(s) 5.4413 | Accuracy: 74.460927 %\n",
            "Epoch 00004 | Loss 0.6696 | Time(s) 5.4539 | Accuracy: 69.911301 %\n",
            "Epoch 00005 | Loss 0.5922 | Time(s) 5.4325 | Accuracy: 77.841923 %\n",
            "Epoch 00006 | Loss 0.8189 | Time(s) 5.4148 | Accuracy: 80.567875 %\n",
            "Epoch 00007 | Loss 0.5943 | Time(s) 5.4052 | Accuracy: 79.465515 %\n",
            "Epoch 00008 | Loss 0.5991 | Time(s) 5.3983 | Accuracy: 76.371260 %\n",
            "Epoch 00009 | Loss 0.6915 | Time(s) 5.3992 | Accuracy: 78.236988 %\n",
            "Epoch 00010 | Loss 0.5498 | Time(s) 5.4050 | Accuracy: 80.098894 %\n",
            "Epoch 00011 | Loss 0.5013 | Time(s) 5.4183 | Accuracy: 78.289239 %\n",
            "Epoch 00012 | Loss 0.4037 | Time(s) 5.4136 | Accuracy: 79.907733 %\n",
            "Epoch 00013 | Loss 0.4949 | Time(s) 5.4084 | Accuracy: 80.615028 %\n",
            "Epoch 00014 | Loss 0.4507 | Time(s) 5.4113 | Accuracy: 81.610338 %\n",
            "Epoch 00015 | Loss 0.4220 | Time(s) 5.4092 | Accuracy: 82.071673 %\n",
            "Epoch 00016 | Loss 0.4129 | Time(s) 5.4126 | Accuracy: 81.914921 %\n",
            "Epoch 00017 | Loss 0.4269 | Time(s) 5.4060 | Accuracy: 82.154509 %\n",
            "Epoch 00018 | Loss 0.4034 | Time(s) 5.4007 | Accuracy: 82.094612 %\n",
            "Epoch 00019 | Loss 0.3888 | Time(s) 5.4012 | Accuracy: 82.238620 %\n",
            "Epoch 00020 | Loss 0.4790 | Time(s) 5.4091 | Accuracy: 82.524086 %\n",
            "Epoch 00021 | Loss 0.3984 | Time(s) 5.4151 | Accuracy: 82.084417 %\n",
            "Epoch 00022 | Loss 0.3970 | Time(s) 5.4154 | Accuracy: 82.814651 %\n",
            "Epoch 00023 | Loss 0.2981 | Time(s) 5.4100 | Accuracy: 82.724168 %\n",
            "Epoch 00024 | Loss 0.3589 | Time(s) 5.4068 | Accuracy: 82.403018 %\n",
            "Epoch 00025 | Loss 0.3161 | Time(s) 5.4054 | Accuracy: 82.680838 %\n",
            "Epoch 00026 | Loss 0.3377 | Time(s) 5.4033 | Accuracy: 82.529184 %\n",
            "Epoch 00027 | Loss 0.5478 | Time(s) 5.4005 | Accuracy: 81.885609 %\n",
            "Epoch 00028 | Loss 0.3873 | Time(s) 5.3968 | Accuracy: 82.072947 %\n",
            "Epoch 00029 | Loss 0.2889 | Time(s) 5.3918 | Accuracy: 81.883061 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0789 | Time(s) 5.1978 | Accuracy: 20.805934 %\n",
            "Epoch 00001 | Loss 1.0138 | Time(s) 5.2284 | Accuracy: 20.804659 %\n",
            "Epoch 00002 | Loss 1.0394 | Time(s) 5.2322 | Accuracy: 20.811031 %\n",
            "Epoch 00003 | Loss 0.9844 | Time(s) 5.2739 | Accuracy: 20.814854 %\n",
            "Epoch 00004 | Loss 1.0252 | Time(s) 5.2567 | Accuracy: 20.818678 %\n",
            "Epoch 00005 | Loss 0.9538 | Time(s) 5.2724 | Accuracy: 20.802110 %\n",
            "Epoch 00006 | Loss 0.9975 | Time(s) 5.2982 | Accuracy: 20.812306 %\n",
            "Epoch 00007 | Loss 0.9454 | Time(s) 5.3031 | Accuracy: 20.803385 %\n",
            "Epoch 00008 | Loss 0.9388 | Time(s) 5.3069 | Accuracy: 20.812306 %\n",
            "Epoch 00009 | Loss 0.9547 | Time(s) 5.3045 | Accuracy: 20.818678 %\n",
            "Epoch 00010 | Loss 0.9487 | Time(s) 5.3034 | Accuracy: 20.809757 %\n",
            "Epoch 00011 | Loss 0.9642 | Time(s) 5.3013 | Accuracy: 20.819952 %\n",
            "Epoch 00012 | Loss 0.9657 | Time(s) 5.3094 | Accuracy: 20.816129 %\n",
            "Epoch 00013 | Loss 0.9347 | Time(s) 5.3089 | Accuracy: 20.818678 %\n",
            "Epoch 00014 | Loss 0.9445 | Time(s) 5.3120 | Accuracy: 20.805934 %\n",
            "Epoch 00015 | Loss 0.9546 | Time(s) 5.3129 | Accuracy: 20.805934 %\n",
            "Epoch 00016 | Loss 0.9533 | Time(s) 5.3049 | Accuracy: 20.813580 %\n",
            "Epoch 00017 | Loss 0.9150 | Time(s) 5.2982 | Accuracy: 20.809757 %\n",
            "Epoch 00018 | Loss 0.9264 | Time(s) 5.2990 | Accuracy: 20.803385 %\n",
            "Epoch 00019 | Loss 0.9224 | Time(s) 5.3007 | Accuracy: 20.809757 %\n",
            "Epoch 00020 | Loss 0.9227 | Time(s) 5.2916 | Accuracy: 20.816129 %\n",
            "Epoch 00021 | Loss 0.9516 | Time(s) 5.2912 | Accuracy: 20.821226 %\n",
            "Epoch 00022 | Loss 0.9428 | Time(s) 5.2975 | Accuracy: 20.804659 %\n",
            "Epoch 00023 | Loss 0.9131 | Time(s) 5.3034 | Accuracy: 20.814854 %\n",
            "Epoch 00024 | Loss 0.9603 | Time(s) 5.3077 | Accuracy: 20.814854 %\n",
            "Epoch 00025 | Loss 0.9223 | Time(s) 5.3129 | Accuracy: 20.814854 %\n",
            "Epoch 00026 | Loss 0.9433 | Time(s) 5.3274 | Accuracy: 20.814854 %\n",
            "Epoch 00027 | Loss 0.9152 | Time(s) 5.3323 | Accuracy: 20.805934 %\n",
            "Epoch 00028 | Loss 0.9356 | Time(s) 5.3312 | Accuracy: 20.812306 %\n",
            "Epoch 00029 | Loss 0.9374 | Time(s) 5.3356 | Accuracy: 20.809757 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0457 | Time(s) 5.7224 | Accuracy: 39.170108 %\n",
            "Epoch 00001 | Loss 1.0243 | Time(s) 5.6492 | Accuracy: 51.528011 %\n",
            "Epoch 00002 | Loss 0.8914 | Time(s) 5.6015 | Accuracy: 53.560687 %\n",
            "Epoch 00003 | Loss 0.7632 | Time(s) 5.5795 | Accuracy: 54.106132 %\n",
            "Epoch 00004 | Loss 0.7914 | Time(s) 5.5815 | Accuracy: 54.563644 %\n",
            "Epoch 00005 | Loss 0.7928 | Time(s) 5.5535 | Accuracy: 54.805781 %\n",
            "Epoch 00006 | Loss 0.7255 | Time(s) 5.5481 | Accuracy: 55.245450 %\n",
            "Epoch 00007 | Loss 0.6907 | Time(s) 5.5264 | Accuracy: 55.309171 %\n",
            "Epoch 00008 | Loss 0.7398 | Time(s) 5.5051 | Accuracy: 55.185553 %\n",
            "Epoch 00009 | Loss 0.7436 | Time(s) 5.4936 | Accuracy: 55.227609 %\n",
            "Epoch 00010 | Loss 1.1140 | Time(s) 5.4903 | Accuracy: 55.207218 %\n",
            "Epoch 00011 | Loss 0.7253 | Time(s) 5.4861 | Accuracy: 54.159657 %\n",
            "Epoch 00012 | Loss 0.9995 | Time(s) 5.4871 | Accuracy: 54.315135 %\n",
            "Epoch 00013 | Loss 0.6171 | Time(s) 5.4790 | Accuracy: 54.710200 %\n",
            "Epoch 00014 | Loss 0.6928 | Time(s) 5.4779 | Accuracy: 55.005862 %\n",
            "Epoch 00015 | Loss 0.6906 | Time(s) 5.4801 | Accuracy: 55.115461 %\n",
            "Epoch 00016 | Loss 0.6339 | Time(s) 5.4801 | Accuracy: 55.086150 %\n",
            "Epoch 00017 | Loss 0.6727 | Time(s) 5.4801 | Accuracy: 54.677066 %\n",
            "Epoch 00018 | Loss 0.6234 | Time(s) 5.4750 | Accuracy: 54.293470 %\n",
            "Epoch 00019 | Loss 0.9018 | Time(s) 5.4691 | Accuracy: 54.745884 %\n",
            "Epoch 00020 | Loss 0.6421 | Time(s) 5.4670 | Accuracy: 54.719121 %\n",
            "Epoch 00021 | Loss 0.5837 | Time(s) 5.4615 | Accuracy: 54.169853 %\n",
            "Epoch 00022 | Loss 0.6432 | Time(s) 5.4619 | Accuracy: 54.118876 %\n",
            "Epoch 00023 | Loss 0.7073 | Time(s) 5.4619 | Accuracy: 54.527960 %\n",
            "Epoch 00024 | Loss 0.6888 | Time(s) 5.4580 | Accuracy: 53.495693 %\n",
            "Epoch 00025 | Loss 0.5913 | Time(s) 5.4517 | Accuracy: 54.446399 %\n",
            "Epoch 00026 | Loss 0.6630 | Time(s) 5.4497 | Accuracy: 54.345721 %\n",
            "Epoch 00027 | Loss 0.6981 | Time(s) 5.4523 | Accuracy: 54.025845 %\n",
            "Epoch 00028 | Loss 0.5919 | Time(s) 5.4532 | Accuracy: 54.202987 %\n",
            "Epoch 00029 | Loss 0.6202 | Time(s) 5.4534 | Accuracy: 54.603150 %\n",
            "Results stored.\n",
            "New learning rate...\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0716 | Time(s) 5.4142 | Accuracy: 40.566855 %\n",
            "Epoch 00001 | Loss 1.0175 | Time(s) 5.5262 | Accuracy: 43.607585 %\n",
            "Epoch 00002 | Loss 0.9802 | Time(s) 5.5275 | Accuracy: 47.829689 %\n",
            "Epoch 00003 | Loss 0.9774 | Time(s) 5.5229 | Accuracy: 51.729367 %\n",
            "Epoch 00004 | Loss 0.9249 | Time(s) 5.5146 | Accuracy: 56.923842 %\n",
            "Epoch 00005 | Loss 0.9182 | Time(s) 5.5043 | Accuracy: 62.608962 %\n",
            "Epoch 00006 | Loss 0.8674 | Time(s) 5.5303 | Accuracy: 65.170515 %\n",
            "Epoch 00007 | Loss 0.8482 | Time(s) 5.5263 | Accuracy: 67.228679 %\n",
            "Epoch 00008 | Loss 0.8667 | Time(s) 5.5432 | Accuracy: 69.911301 %\n",
            "Epoch 00009 | Loss 0.8014 | Time(s) 5.5651 | Accuracy: 70.565071 %\n",
            "Epoch 00010 | Loss 0.7772 | Time(s) 5.5842 | Accuracy: 70.554876 %\n",
            "Epoch 00011 | Loss 0.7156 | Time(s) 5.5907 | Accuracy: 72.200133 %\n",
            "Epoch 00012 | Loss 0.7118 | Time(s) 5.5995 | Accuracy: 75.080288 %\n",
            "Epoch 00013 | Loss 0.7023 | Time(s) 5.6032 | Accuracy: 76.307539 %\n",
            "Epoch 00014 | Loss 0.6671 | Time(s) 5.6144 | Accuracy: 76.632513 %\n",
            "Epoch 00015 | Loss 0.7005 | Time(s) 5.6237 | Accuracy: 76.250191 %\n",
            "Epoch 00016 | Loss 0.6626 | Time(s) 5.6308 | Accuracy: 76.751032 %\n",
            "Epoch 00017 | Loss 0.6542 | Time(s) 5.6445 | Accuracy: 77.909466 %\n",
            "Epoch 00018 | Loss 0.6366 | Time(s) 5.6554 | Accuracy: 78.962125 %\n",
            "Epoch 00019 | Loss 0.7388 | Time(s) 5.6636 | Accuracy: 79.578937 %\n",
            "Epoch 00020 | Loss 0.5945 | Time(s) 5.6764 | Accuracy: 79.599327 %\n",
            "Epoch 00021 | Loss 0.5827 | Time(s) 5.6862 | Accuracy: 79.714024 %\n",
            "Epoch 00022 | Loss 0.6065 | Time(s) 5.7005 | Accuracy: 80.109089 %\n",
            "Epoch 00023 | Loss 0.5728 | Time(s) 5.7052 | Accuracy: 80.030076 %\n",
            "Epoch 00024 | Loss 0.5941 | Time(s) 5.7126 | Accuracy: 79.921752 %\n",
            "Epoch 00025 | Loss 0.5786 | Time(s) 5.7169 | Accuracy: 80.451904 %\n",
            "Epoch 00026 | Loss 0.5688 | Time(s) 5.7197 | Accuracy: 80.714431 %\n",
            "Epoch 00027 | Loss 0.5659 | Time(s) 5.7263 | Accuracy: 81.138808 %\n",
            "Epoch 00028 | Loss 0.5557 | Time(s) 5.7330 | Accuracy: 81.166845 %\n",
            "Epoch 00029 | Loss 0.5351 | Time(s) 5.7382 | Accuracy: 81.058521 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0582 | Time(s) 5.8288 | Accuracy: 39.698986 %\n",
            "Epoch 00001 | Loss 0.9993 | Time(s) 5.8396 | Accuracy: 57.847785 %\n",
            "Epoch 00002 | Loss 0.9497 | Time(s) 5.8936 | Accuracy: 64.196870 %\n",
            "Epoch 00003 | Loss 0.9147 | Time(s) 5.8877 | Accuracy: 68.132232 %\n",
            "Epoch 00004 | Loss 0.8763 | Time(s) 5.8787 | Accuracy: 71.004741 %\n",
            "Epoch 00005 | Loss 0.8422 | Time(s) 5.8689 | Accuracy: 73.266809 %\n",
            "Epoch 00006 | Loss 0.8650 | Time(s) 5.8661 | Accuracy: 74.830504 %\n",
            "Epoch 00007 | Loss 0.8326 | Time(s) 5.8671 | Accuracy: 76.002957 %\n",
            "Epoch 00008 | Loss 0.8398 | Time(s) 5.8689 | Accuracy: 76.136769 %\n",
            "Epoch 00009 | Loss 0.7779 | Time(s) 5.8663 | Accuracy: 77.426467 %\n",
            "Epoch 00010 | Loss 0.7256 | Time(s) 5.8811 | Accuracy: 78.755671 %\n",
            "Epoch 00011 | Loss 0.7711 | Time(s) 5.8716 | Accuracy: 79.459143 %\n",
            "Epoch 00012 | Loss 0.6931 | Time(s) 5.8795 | Accuracy: 79.858031 %\n",
            "Epoch 00013 | Loss 0.6624 | Time(s) 5.8821 | Accuracy: 79.694907 %\n",
            "Epoch 00014 | Loss 0.6545 | Time(s) 5.8852 | Accuracy: 79.845287 %\n",
            "Epoch 00015 | Loss 0.6489 | Time(s) 5.8801 | Accuracy: 80.153693 %\n",
            "Epoch 00016 | Loss 0.5986 | Time(s) 5.8766 | Accuracy: 79.905184 %\n",
            "Epoch 00017 | Loss 0.5963 | Time(s) 5.8724 | Accuracy: 80.223785 %\n",
            "Epoch 00018 | Loss 0.6395 | Time(s) 5.8720 | Accuracy: 80.204669 %\n",
            "Epoch 00019 | Loss 0.5726 | Time(s) 5.8791 | Accuracy: 79.714024 %\n",
            "Epoch 00020 | Loss 0.6063 | Time(s) 5.8773 | Accuracy: 79.775195 %\n",
            "Epoch 00021 | Loss 0.6120 | Time(s) 5.8736 | Accuracy: 80.803640 %\n",
            "Epoch 00022 | Loss 0.5368 | Time(s) 5.8692 | Accuracy: 81.284090 %\n",
            "Epoch 00023 | Loss 0.5656 | Time(s) 5.8675 | Accuracy: 81.089106 %\n",
            "Epoch 00024 | Loss 0.5881 | Time(s) 5.8664 | Accuracy: 80.792170 %\n",
            "Epoch 00025 | Loss 0.5246 | Time(s) 5.8655 | Accuracy: 80.789621 %\n",
            "Epoch 00026 | Loss 0.5465 | Time(s) 5.8629 | Accuracy: 81.202528 %\n",
            "Epoch 00027 | Loss 0.5874 | Time(s) 5.8601 | Accuracy: 81.168119 %\n",
            "Epoch 00028 | Loss 0.5068 | Time(s) 5.8608 | Accuracy: 81.257328 %\n",
            "Epoch 00029 | Loss 0.5295 | Time(s) 5.8613 | Accuracy: 81.187236 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0289 | Time(s) 5.9553 | Accuracy: 43.252026 %\n",
            "Epoch 00001 | Loss 0.9494 | Time(s) 5.9041 | Accuracy: 52.429016 %\n",
            "Epoch 00002 | Loss 0.9168 | Time(s) 5.8760 | Accuracy: 55.733547 %\n",
            "Epoch 00003 | Loss 0.8766 | Time(s) 5.8924 | Accuracy: 58.059336 %\n",
            "Epoch 00004 | Loss 0.8715 | Time(s) 5.8934 | Accuracy: 61.000663 %\n",
            "Epoch 00005 | Loss 0.8088 | Time(s) 5.8769 | Accuracy: 66.396493 %\n",
            "Epoch 00006 | Loss 0.8627 | Time(s) 5.8703 | Accuracy: 71.878983 %\n",
            "Epoch 00007 | Loss 0.7998 | Time(s) 5.8816 | Accuracy: 74.728552 %\n",
            "Epoch 00008 | Loss 0.7408 | Time(s) 5.8879 | Accuracy: 75.893358 %\n",
            "Epoch 00009 | Loss 0.7842 | Time(s) 5.8790 | Accuracy: 76.729367 %\n",
            "Epoch 00010 | Loss 0.7209 | Time(s) 5.8730 | Accuracy: 77.934954 %\n",
            "Epoch 00011 | Loss 0.7520 | Time(s) 5.8691 | Accuracy: 78.814294 %\n",
            "Epoch 00012 | Loss 0.7332 | Time(s) 5.8745 | Accuracy: 78.684304 %\n",
            "Epoch 00013 | Loss 0.6744 | Time(s) 5.8773 | Accuracy: 77.353826 %\n",
            "Epoch 00014 | Loss 0.7726 | Time(s) 5.8932 | Accuracy: 76.624866 %\n",
            "Epoch 00015 | Loss 0.6396 | Time(s) 5.8998 | Accuracy: 78.254830 %\n",
            "Epoch 00016 | Loss 0.6333 | Time(s) 5.8893 | Accuracy: 80.434062 %\n",
            "Epoch 00017 | Loss 0.6201 | Time(s) 5.8853 | Accuracy: 80.639241 %\n",
            "Epoch 00018 | Loss 0.6316 | Time(s) 5.8891 | Accuracy: 80.146047 %\n",
            "Epoch 00019 | Loss 0.6064 | Time(s) 5.8896 | Accuracy: 79.339349 %\n",
            "Epoch 00020 | Loss 0.6061 | Time(s) 5.8903 | Accuracy: 79.772646 %\n",
            "Epoch 00021 | Loss 0.5845 | Time(s) 5.8839 | Accuracy: 80.504155 %\n",
            "Epoch 00022 | Loss 0.6367 | Time(s) 5.8829 | Accuracy: 80.905592 %\n",
            "Epoch 00023 | Loss 0.5762 | Time(s) 5.8830 | Accuracy: 81.007544 %\n",
            "Epoch 00024 | Loss 0.6498 | Time(s) 5.8863 | Accuracy: 81.090381 %\n",
            "Epoch 00025 | Loss 0.6256 | Time(s) 5.8847 | Accuracy: 81.257328 %\n",
            "Epoch 00026 | Loss 0.5471 | Time(s) 5.8831 | Accuracy: 81.281542 %\n",
            "Epoch 00027 | Loss 0.5221 | Time(s) 5.8826 | Accuracy: 80.980782 %\n",
            "Epoch 00028 | Loss 0.5494 | Time(s) 5.8845 | Accuracy: 81.183412 %\n",
            "Epoch 00029 | Loss 0.5926 | Time(s) 5.8862 | Accuracy: 81.573380 %\n",
            "Results stored.\n",
            "New learning rate...\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0766 | Time(s) 5.9196 | Accuracy: 41.944487 %\n",
            "Epoch 00001 | Loss 1.0717 | Time(s) 5.8464 | Accuracy: 42.790692 %\n",
            "Epoch 00002 | Loss 1.0542 | Time(s) 5.8244 | Accuracy: 43.731203 %\n",
            "Epoch 00003 | Loss 1.0728 | Time(s) 5.7965 | Accuracy: 44.372228 %\n",
            "Epoch 00004 | Loss 1.0529 | Time(s) 5.8054 | Accuracy: 44.935515 %\n",
            "Epoch 00005 | Loss 1.0525 | Time(s) 5.7929 | Accuracy: 45.381557 %\n",
            "Epoch 00006 | Loss 1.0420 | Time(s) 5.7947 | Accuracy: 45.850538 %\n",
            "Epoch 00007 | Loss 1.0406 | Time(s) 5.8003 | Accuracy: 46.412550 %\n",
            "Epoch 00008 | Loss 1.0314 | Time(s) 5.8090 | Accuracy: 47.031911 %\n",
            "Epoch 00009 | Loss 1.0145 | Time(s) 5.7987 | Accuracy: 47.544222 %\n",
            "Epoch 00010 | Loss 1.0092 | Time(s) 5.7954 | Accuracy: 48.034868 %\n",
            "Epoch 00011 | Loss 1.0182 | Time(s) 5.8004 | Accuracy: 48.666973 %\n",
            "Epoch 00012 | Loss 1.0110 | Time(s) 5.7967 | Accuracy: 49.169088 %\n",
            "Epoch 00013 | Loss 1.0025 | Time(s) 5.7943 | Accuracy: 49.603660 %\n",
            "Epoch 00014 | Loss 1.0118 | Time(s) 5.7972 | Accuracy: 49.970689 %\n",
            "Epoch 00015 | Loss 0.9926 | Time(s) 5.7991 | Accuracy: 50.282918 %\n",
            "Epoch 00016 | Loss 0.9899 | Time(s) 5.7980 | Accuracy: 50.540348 %\n",
            "Epoch 00017 | Loss 0.9961 | Time(s) 5.7961 | Accuracy: 50.807973 %\n",
            "Epoch 00018 | Loss 0.9836 | Time(s) 5.7955 | Accuracy: 51.201764 %\n",
            "Epoch 00019 | Loss 0.9788 | Time(s) 5.7975 | Accuracy: 51.623592 %\n",
            "Epoch 00020 | Loss 0.9722 | Time(s) 5.7992 | Accuracy: 51.949839 %\n",
            "Epoch 00021 | Loss 0.9812 | Time(s) 5.7990 | Accuracy: 52.342356 %\n",
            "Epoch 00022 | Loss 0.9684 | Time(s) 5.7970 | Accuracy: 52.565377 %\n",
            "Epoch 00023 | Loss 0.9645 | Time(s) 5.8005 | Accuracy: 52.770556 %\n",
            "Epoch 00024 | Loss 0.9584 | Time(s) 5.7990 | Accuracy: 53.020340 %\n",
            "Epoch 00025 | Loss 0.9567 | Time(s) 5.7986 | Accuracy: 53.383545 %\n",
            "Epoch 00026 | Loss 0.9499 | Time(s) 5.7944 | Accuracy: 53.899679 %\n",
            "Epoch 00027 | Loss 0.9471 | Time(s) 5.7910 | Accuracy: 54.359739 %\n",
            "Epoch 00028 | Loss 0.9358 | Time(s) 5.7900 | Accuracy: 54.766274 %\n",
            "Epoch 00029 | Loss 0.9395 | Time(s) 5.7908 | Accuracy: 55.128205 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0974 | Time(s) 5.6543 | Accuracy: 36.116634 %\n",
            "Epoch 00001 | Loss 1.0936 | Time(s) 5.7142 | Accuracy: 37.151450 %\n",
            "Epoch 00002 | Loss 1.0939 | Time(s) 5.7090 | Accuracy: 38.509966 %\n",
            "Epoch 00003 | Loss 1.0899 | Time(s) 5.7001 | Accuracy: 40.066014 %\n",
            "Epoch 00004 | Loss 1.0947 | Time(s) 5.7199 | Accuracy: 41.559617 %\n",
            "Epoch 00005 | Loss 1.0767 | Time(s) 5.7250 | Accuracy: 42.557476 %\n",
            "Epoch 00006 | Loss 1.0778 | Time(s) 5.7240 | Accuracy: 43.364174 %\n",
            "Epoch 00007 | Loss 1.0654 | Time(s) 5.7300 | Accuracy: 43.977163 %\n",
            "Epoch 00008 | Loss 1.0585 | Time(s) 5.7410 | Accuracy: 44.317429 %\n",
            "Epoch 00009 | Loss 1.0437 | Time(s) 5.7392 | Accuracy: 44.618188 %\n",
            "Epoch 00010 | Loss 1.0462 | Time(s) 5.7344 | Accuracy: 44.781312 %\n",
            "Epoch 00011 | Loss 1.0327 | Time(s) 5.7449 | Accuracy: 45.034919 %\n",
            "Epoch 00012 | Loss 1.0276 | Time(s) 5.7438 | Accuracy: 45.581638 %\n",
            "Epoch 00013 | Loss 1.0517 | Time(s) 5.7372 | Accuracy: 46.209920 %\n",
            "Epoch 00014 | Loss 1.0286 | Time(s) 5.7375 | Accuracy: 46.835653 %\n",
            "Epoch 00015 | Loss 1.0301 | Time(s) 5.7385 | Accuracy: 47.767243 %\n",
            "Epoch 00016 | Loss 1.0159 | Time(s) 5.7412 | Accuracy: 48.511495 %\n",
            "Epoch 00017 | Loss 1.0094 | Time(s) 5.7450 | Accuracy: 49.250650 %\n",
            "Epoch 00018 | Loss 1.0113 | Time(s) 5.7424 | Accuracy: 49.937554 %\n",
            "Epoch 00019 | Loss 0.9983 | Time(s) 5.7415 | Accuracy: 50.410358 %\n",
            "Epoch 00020 | Loss 1.0159 | Time(s) 5.7400 | Accuracy: 50.957078 %\n",
            "Epoch 00021 | Loss 0.9884 | Time(s) 5.7372 | Accuracy: 51.526737 %\n",
            "Epoch 00022 | Loss 0.9854 | Time(s) 5.7385 | Accuracy: 52.139726 %\n",
            "Epoch 00023 | Loss 0.9941 | Time(s) 5.7300 | Accuracy: 52.784575 %\n",
            "Epoch 00024 | Loss 0.9922 | Time(s) 5.7256 | Accuracy: 53.632054 %\n",
            "Epoch 00025 | Loss 0.9652 | Time(s) 5.7232 | Accuracy: 54.364837 %\n",
            "Epoch 00026 | Loss 0.9927 | Time(s) 5.7251 | Accuracy: 55.106540 %\n",
            "Epoch 00027 | Loss 0.9668 | Time(s) 5.7233 | Accuracy: 55.794719 %\n",
            "Epoch 00028 | Loss 0.9503 | Time(s) 5.7241 | Accuracy: 56.210175 %\n",
            "Epoch 00029 | Loss 0.9451 | Time(s) 5.7183 | Accuracy: 56.601417 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0908 | Time(s) 5.5769 | Accuracy: 39.683693 %\n",
            "Epoch 00001 | Loss 1.0906 | Time(s) 5.5814 | Accuracy: 39.577917 %\n",
            "Epoch 00002 | Loss 1.0838 | Time(s) 5.5883 | Accuracy: 39.329408 %\n",
            "Epoch 00003 | Loss 1.0839 | Time(s) 5.5833 | Accuracy: 39.055411 %\n",
            "Epoch 00004 | Loss 1.0789 | Time(s) 5.5689 | Accuracy: 39.000612 %\n",
            "Epoch 00005 | Loss 1.0656 | Time(s) 5.5819 | Accuracy: 39.593210 %\n",
            "Epoch 00006 | Loss 1.0641 | Time(s) 5.5742 | Accuracy: 40.713412 %\n",
            "Epoch 00007 | Loss 1.0741 | Time(s) 5.5753 | Accuracy: 42.047714 %\n",
            "Epoch 00008 | Loss 1.0503 | Time(s) 5.5709 | Accuracy: 43.164092 %\n",
            "Epoch 00009 | Loss 1.0453 | Time(s) 5.5792 | Accuracy: 44.133914 %\n",
            "Epoch 00010 | Loss 1.0415 | Time(s) 5.5876 | Accuracy: 45.037468 %\n",
            "Epoch 00011 | Loss 1.0363 | Time(s) 5.5816 | Accuracy: 45.877300 %\n",
            "Epoch 00012 | Loss 1.0239 | Time(s) 5.5864 | Accuracy: 46.687822 %\n",
            "Epoch 00013 | Loss 1.0233 | Time(s) 5.5921 | Accuracy: 47.379824 %\n",
            "Epoch 00014 | Loss 1.0205 | Time(s) 5.5863 | Accuracy: 48.153387 %\n",
            "Epoch 00015 | Loss 1.0151 | Time(s) 5.5820 | Accuracy: 48.846664 %\n",
            "Epoch 00016 | Loss 1.0156 | Time(s) 5.5848 | Accuracy: 49.589642 %\n",
            "Epoch 00017 | Loss 1.0015 | Time(s) 5.5850 | Accuracy: 50.243411 %\n",
            "Epoch 00018 | Loss 1.0029 | Time(s) 5.5840 | Accuracy: 50.858949 %\n",
            "Epoch 00019 | Loss 0.9944 | Time(s) 5.5802 | Accuracy: 51.418413 %\n",
            "Epoch 00020 | Loss 0.9853 | Time(s) 5.5779 | Accuracy: 52.050517 %\n",
            "Epoch 00021 | Loss 0.9922 | Time(s) 5.5787 | Accuracy: 52.663506 %\n",
            "Epoch 00022 | Loss 0.9726 | Time(s) 5.5779 | Accuracy: 53.341490 %\n",
            "Epoch 00023 | Loss 1.0133 | Time(s) 5.5753 | Accuracy: 53.869093 %\n",
            "Epoch 00024 | Loss 0.9538 | Time(s) 5.5787 | Accuracy: 54.354641 %\n",
            "Epoch 00025 | Loss 0.9654 | Time(s) 5.5775 | Accuracy: 54.846562 %\n",
            "Epoch 00026 | Loss 0.9449 | Time(s) 5.5808 | Accuracy: 55.209767 %\n",
            "Epoch 00027 | Loss 0.9406 | Time(s) 5.5814 | Accuracy: 55.612479 %\n",
            "Epoch 00028 | Loss 0.9603 | Time(s) 5.5815 | Accuracy: 56.016465 %\n",
            "Epoch 00029 | Loss 0.9545 | Time(s) 5.5849 | Accuracy: 56.343987 %\n",
            "Results stored.\n",
            "New learning rate...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Simlx8ur7sgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "axisX = range(30)\n",
        "colors = dict()\n",
        "#lrs = [1e-1, 1e-2, 1e-3, 1e-4]\n",
        "colors[1e-1] = \"red\"\n",
        "colors[1e-2] = \"orange\"\n",
        "colors[1e-3] = \"blue\"\n",
        "colors[1e-4] = \"green\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BcuvuOK9ALs",
        "colab_type": "code",
        "outputId": "8e340a54-0957-4706-c949-c47678e29238",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "for lr in lrs:\n",
        "  plt.plot(axisX, lrAccTest[lr], color = colors[lr], label=str(lr))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpQULOcc8fka",
        "colab_type": "code",
        "outputId": "c9454726-5b3a-4d44-ed3c-870bdfd29200",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "for lr in lrs:\n",
        "  plt.plot(axisX, lrLossTest[lr], color = colors[lr], label=str(lr))\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAD4CAYAAAATpHZ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70XUAV7w5Jsf",
        "colab_type": "text"
      },
      "source": [
        "TEST ^^^^^^^^^^^^^^^^^^^^^^^^^"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3SQOrzeN3mo",
        "colab_type": "code",
        "outputId": "36964676-13d4-4f33-c2f6-f833b60b8857",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "import collections\n",
        "import numpy as np\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#init counters\n",
        "averageAccPubMed = []\n",
        "averageLossPubMed = []\n",
        "\n",
        "for t in range(n_of_training_cycles):\n",
        "  print(\"Starting new training\")\n",
        "  #get data from DGL\n",
        "  net = Net()\n",
        "  g, features, labels, mask = load_pubmed_data()\n",
        "  print(g)\n",
        "\n",
        "  #point to show on graph\n",
        "  pointsPubMed=dict()\n",
        "  pointsLossPubMed=dict()\n",
        "\n",
        "  #the number of masks to get when training per epoch\n",
        "  n_masks_to_try = 4\n",
        "  #initializing the optimizer (optimizer takes care of optimize the learining rate during training)\n",
        "  optimizer = th.optim.Adam(net.parameters(), lr=2e-3)\n",
        "  optimizer.state = collections.defaultdict(dict)\n",
        "\n",
        "  #dur is just an array to store the duration in order to show them later\n",
        "  dur = []\n",
        "\n",
        "  #this 'for' cycles on 200 epochs\n",
        "  for epoch in range(n_of_epochs):\n",
        "      t0 = time.time()\n",
        "\n",
        "      #getting only some masks (4)\n",
        "      masksToTry = random.sample(masks,n_masks_to_try)\n",
        "\n",
        "      for m in masksToTry:\n",
        "          #calling 'net(...)' it asks to the GCN to compute the forward    \n",
        "\n",
        "          logits = net(g, features)\n",
        "          logp = F.log_softmax(logits, 1)\n",
        "\n",
        "          #compute loss like the negative log likelihood loss\n",
        "          loss = F.nll_loss(logp[m], labels[m])\n",
        "          \n",
        "          #Since the backward() function accumulates gradients, and you \n",
        "          #don’t want to mix up gradients between minibatch, you have \n",
        "          #to zero them out at the start of a new minibatch. This is \n",
        "          #exactly like how a general (additive) accumulator variable is \n",
        "          #initialized to 0 in code.\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          #update network weights by loss\n",
        "          loss.backward()\n",
        "\n",
        "          #update optimizer's values after backward\n",
        "          optimizer.step()\n",
        "\n",
        "          #computing accuracy\n",
        "          i = 0\n",
        "          matched = 0\n",
        "          while i < 19717:\n",
        "            if m[i] == 0:\n",
        "              #getting index of the maximum\n",
        "              j = 0\n",
        "              max = None\n",
        "              jMax = 0\n",
        "              for a in logp[i]:\n",
        "                if max==None:\n",
        "                  max = a.item()\n",
        "                  jMax = j\n",
        "                elif max < a.item():\n",
        "                  max = a.item()\n",
        "                  jMax = j\n",
        "                j = j + 1\n",
        "              if jMax == labels[i]:\n",
        "                matched = matched + 1\n",
        "            i = i + 1\n",
        "          acc = matched/(19717-size_masks)*100\n",
        "\n",
        "          if epoch not in pointsPubMed:\n",
        "            pointsPubMed[epoch] = 0\n",
        "            pointsLossPubMed[epoch] = 0\n",
        "          pointsPubMed[epoch] = pointsPubMed[epoch] + acc\n",
        "          pointsLossPubMed[epoch] = pointsLossPubMed[epoch] + loss.item()\n",
        "          \n",
        "      dur.append(time.time() - t0)\n",
        "      \n",
        "      #computing the average of the accuracy and the loss\n",
        "      pointsPubMed[epoch] = pointsPubMed[epoch]/n_masks_to_try\n",
        "      pointsLossPubMed[epoch] = pointsLossPubMed[epoch]/n_masks_to_try\n",
        "\n",
        "      print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f} | Accuracy: {:.6f} %\".format(\n",
        "              epoch, loss.item(), np.mean(dur), pointsPubMed[epoch]))\n",
        "  #storing results    \n",
        "  averageAccPubMed.append(pointsPubMed)\n",
        "  averageLossPubMed.append(pointsLossPubMed)\n",
        "  print(\"Results stored.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0122 | Time(s) 5.4713 | Accuracy: 44.661518 %\n",
            "Epoch 00001 | Loss 0.9039 | Time(s) 5.4781 | Accuracy: 50.350461 %\n",
            "Epoch 00002 | Loss 0.8905 | Time(s) 5.4659 | Accuracy: 56.586124 %\n",
            "Epoch 00003 | Loss 0.7999 | Time(s) 5.4654 | Accuracy: 61.882551 %\n",
            "Epoch 00004 | Loss 0.7733 | Time(s) 5.4543 | Accuracy: 65.589795 %\n",
            "Epoch 00005 | Loss 0.7454 | Time(s) 5.4639 | Accuracy: 67.989499 %\n",
            "Epoch 00006 | Loss 0.7854 | Time(s) 5.4647 | Accuracy: 69.048529 %\n",
            "Epoch 00007 | Loss 0.6895 | Time(s) 5.4849 | Accuracy: 71.945252 %\n",
            "Epoch 00008 | Loss 0.7206 | Time(s) 5.4777 | Accuracy: 75.039507 %\n",
            "Epoch 00009 | Loss 0.6326 | Time(s) 5.4781 | Accuracy: 73.919305 %\n",
            "Epoch 00010 | Loss 0.6149 | Time(s) 5.4836 | Accuracy: 74.186930 %\n",
            "Epoch 00011 | Loss 0.5931 | Time(s) 5.4840 | Accuracy: 75.999133 %\n",
            "Epoch 00012 | Loss 0.5958 | Time(s) 5.4863 | Accuracy: 78.225519 %\n",
            "Epoch 00013 | Loss 0.5553 | Time(s) 5.4893 | Accuracy: 78.953204 %\n",
            "Epoch 00014 | Loss 0.5636 | Time(s) 5.4857 | Accuracy: 78.131213 %\n",
            "Epoch 00015 | Loss 0.5424 | Time(s) 5.4879 | Accuracy: 77.207269 %\n",
            "Epoch 00016 | Loss 0.5216 | Time(s) 5.5054 | Accuracy: 77.589591 %\n",
            "Epoch 00017 | Loss 0.5075 | Time(s) 5.5053 | Accuracy: 78.258653 %\n",
            "Epoch 00018 | Loss 0.5290 | Time(s) 5.5103 | Accuracy: 79.071724 %\n",
            "Epoch 00019 | Loss 0.4692 | Time(s) 5.5107 | Accuracy: 79.032217 %\n",
            "Epoch 00020 | Loss 0.5534 | Time(s) 5.5063 | Accuracy: 78.156701 %\n",
            "Epoch 00021 | Loss 0.5001 | Time(s) 5.5065 | Accuracy: 78.964673 %\n",
            "Epoch 00022 | Loss 0.4946 | Time(s) 5.5081 | Accuracy: 80.003313 %\n",
            "Epoch 00023 | Loss 0.5075 | Time(s) 5.5151 | Accuracy: 80.509252 %\n",
            "Epoch 00024 | Loss 0.4562 | Time(s) 5.5156 | Accuracy: 80.362696 %\n",
            "Epoch 00025 | Loss 0.4756 | Time(s) 5.5188 | Accuracy: 80.631595 %\n",
            "Epoch 00026 | Loss 0.4364 | Time(s) 5.5257 | Accuracy: 80.775603 %\n",
            "Epoch 00027 | Loss 0.4557 | Time(s) 5.5278 | Accuracy: 79.609522 %\n",
            "Epoch 00028 | Loss 0.4396 | Time(s) 5.5315 | Accuracy: 78.345313 %\n",
            "Epoch 00029 | Loss 0.5052 | Time(s) 5.5388 | Accuracy: 78.876740 %\n",
            "Epoch 00030 | Loss 0.4390 | Time(s) 5.5369 | Accuracy: 80.825305 %\n",
            "Epoch 00031 | Loss 0.5065 | Time(s) 5.5352 | Accuracy: 81.335067 %\n",
            "Epoch 00032 | Loss 0.4717 | Time(s) 5.5373 | Accuracy: 80.644339 %\n",
            "Epoch 00033 | Loss 0.4205 | Time(s) 5.5393 | Accuracy: 79.491003 %\n",
            "Epoch 00034 | Loss 0.4266 | Time(s) 5.5414 | Accuracy: 79.903910 %\n",
            "Epoch 00035 | Loss 0.4063 | Time(s) 5.5418 | Accuracy: 81.123515 %\n",
            "Epoch 00036 | Loss 0.4386 | Time(s) 5.5416 | Accuracy: 81.578478 %\n",
            "Epoch 00037 | Loss 0.4500 | Time(s) 5.5427 | Accuracy: 81.879237 %\n",
            "Epoch 00038 | Loss 0.4228 | Time(s) 5.5472 | Accuracy: 82.011776 %\n",
            "Epoch 00039 | Loss 0.6064 | Time(s) 5.5498 | Accuracy: 81.583575 %\n",
            "Epoch 00040 | Loss 0.4281 | Time(s) 5.5494 | Accuracy: 80.556405 %\n",
            "Epoch 00041 | Loss 0.4534 | Time(s) 5.5531 | Accuracy: 80.720803 %\n",
            "Epoch 00042 | Loss 0.4111 | Time(s) 5.5558 | Accuracy: 81.136259 %\n",
            "Epoch 00043 | Loss 0.4989 | Time(s) 5.5582 | Accuracy: 81.805322 %\n",
            "Epoch 00044 | Loss 0.4182 | Time(s) 5.5593 | Accuracy: 82.187643 %\n",
            "Epoch 00045 | Loss 0.4242 | Time(s) 5.5604 | Accuracy: 81.712290 %\n",
            "Epoch 00046 | Loss 0.3811 | Time(s) 5.5596 | Accuracy: 80.979508 %\n",
            "Epoch 00047 | Loss 0.4031 | Time(s) 5.5598 | Accuracy: 80.956568 %\n",
            "Epoch 00048 | Loss 0.5275 | Time(s) 5.5614 | Accuracy: 81.727583 %\n",
            "Epoch 00049 | Loss 0.3435 | Time(s) 5.5604 | Accuracy: 81.851200 %\n",
            "Epoch 00050 | Loss 0.4068 | Time(s) 5.5627 | Accuracy: 81.765815 %\n",
            "Epoch 00051 | Loss 0.4757 | Time(s) 5.5634 | Accuracy: 81.828261 %\n",
            "Epoch 00052 | Loss 0.3639 | Time(s) 5.5645 | Accuracy: 81.856298 %\n",
            "Epoch 00053 | Loss 0.3622 | Time(s) 5.5647 | Accuracy: 81.858847 %\n",
            "Epoch 00054 | Loss 0.3638 | Time(s) 5.5658 | Accuracy: 81.714839 %\n",
            "Epoch 00055 | Loss 0.3775 | Time(s) 5.5662 | Accuracy: 81.806596 %\n",
            "Epoch 00056 | Loss 0.4083 | Time(s) 5.5671 | Accuracy: 81.574655 %\n",
            "Epoch 00057 | Loss 0.4160 | Time(s) 5.5673 | Accuracy: 81.323597 %\n",
            "Epoch 00058 | Loss 0.5429 | Time(s) 5.5673 | Accuracy: 79.986746 %\n",
            "Epoch 00059 | Loss 0.3428 | Time(s) 5.5687 | Accuracy: 81.185961 %\n",
            "Epoch 00060 | Loss 0.3544 | Time(s) 5.5680 | Accuracy: 81.526227 %\n",
            "Epoch 00061 | Loss 0.3542 | Time(s) 5.5706 | Accuracy: 81.616710 %\n",
            "Epoch 00062 | Loss 0.3772 | Time(s) 5.5720 | Accuracy: 81.505837 %\n",
            "Epoch 00063 | Loss 0.3503 | Time(s) 5.5730 | Accuracy: 81.425549 %\n",
            "Epoch 00064 | Loss 0.3988 | Time(s) 5.5730 | Accuracy: 81.459958 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0925 | Time(s) 5.5592 | Accuracy: 38.280573 %\n",
            "Epoch 00001 | Loss 1.0566 | Time(s) 5.5880 | Accuracy: 51.047561 %\n",
            "Epoch 00002 | Loss 0.9707 | Time(s) 5.5707 | Accuracy: 51.146964 %\n",
            "Epoch 00003 | Loss 1.0112 | Time(s) 5.5735 | Accuracy: 47.471581 %\n",
            "Epoch 00004 | Loss 0.9680 | Time(s) 5.5912 | Accuracy: 50.449865 %\n",
            "Epoch 00005 | Loss 1.0799 | Time(s) 5.6014 | Accuracy: 53.141408 %\n",
            "Epoch 00006 | Loss 1.0712 | Time(s) 5.5899 | Accuracy: 54.251415 %\n",
            "Epoch 00007 | Loss 0.9279 | Time(s) 5.5966 | Accuracy: 54.576388 %\n",
            "Epoch 00008 | Loss 1.0548 | Time(s) 5.5815 | Accuracy: 54.018199 %\n",
            "Epoch 00009 | Loss 0.8977 | Time(s) 5.5911 | Accuracy: 53.132487 %\n",
            "Epoch 00010 | Loss 0.8654 | Time(s) 5.5916 | Accuracy: 52.055615 %\n",
            "Epoch 00011 | Loss 1.0409 | Time(s) 5.5893 | Accuracy: 51.747209 %\n",
            "Epoch 00012 | Loss 0.9187 | Time(s) 5.5887 | Accuracy: 51.909058 %\n",
            "Epoch 00013 | Loss 0.8810 | Time(s) 5.5874 | Accuracy: 52.169037 %\n",
            "Epoch 00014 | Loss 0.8465 | Time(s) 5.5872 | Accuracy: 52.481266 %\n",
            "Epoch 00015 | Loss 0.8650 | Time(s) 5.5964 | Accuracy: 52.330886 %\n",
            "Epoch 00016 | Loss 0.8540 | Time(s) 5.6074 | Accuracy: 52.324514 %\n",
            "Epoch 00017 | Loss 0.8927 | Time(s) 5.6059 | Accuracy: 52.454504 %\n",
            "Epoch 00018 | Loss 0.8495 | Time(s) 5.6053 | Accuracy: 53.073865 %\n",
            "Epoch 00019 | Loss 0.8204 | Time(s) 5.6030 | Accuracy: 53.110822 %\n",
            "Epoch 00020 | Loss 0.8272 | Time(s) 5.6082 | Accuracy: 51.967681 %\n",
            "Epoch 00021 | Loss 0.8333 | Time(s) 5.6062 | Accuracy: 51.911607 %\n",
            "Epoch 00022 | Loss 0.8378 | Time(s) 5.6071 | Accuracy: 53.345313 %\n",
            "Epoch 00023 | Loss 0.8871 | Time(s) 5.6095 | Accuracy: 53.630779 %\n",
            "Epoch 00024 | Loss 0.8814 | Time(s) 5.6057 | Accuracy: 53.194933 %\n",
            "Epoch 00025 | Loss 0.8174 | Time(s) 5.6083 | Accuracy: 51.719172 %\n",
            "Epoch 00026 | Loss 0.8681 | Time(s) 5.6049 | Accuracy: 51.759953 %\n",
            "Epoch 00027 | Loss 0.8649 | Time(s) 5.6057 | Accuracy: 52.924759 %\n",
            "Epoch 00028 | Loss 0.9893 | Time(s) 5.6071 | Accuracy: 52.991028 %\n",
            "Epoch 00029 | Loss 0.7728 | Time(s) 5.6096 | Accuracy: 52.915838 %\n",
            "Epoch 00030 | Loss 0.7985 | Time(s) 5.6106 | Accuracy: 52.890350 %\n",
            "Epoch 00031 | Loss 0.7796 | Time(s) 5.6149 | Accuracy: 52.671153 %\n",
            "Epoch 00032 | Loss 0.7738 | Time(s) 5.6179 | Accuracy: 52.427741 %\n",
            "Epoch 00033 | Loss 0.8693 | Time(s) 5.6176 | Accuracy: 52.343631 %\n",
            "Epoch 00034 | Loss 0.8825 | Time(s) 5.6171 | Accuracy: 52.938778 %\n",
            "Epoch 00035 | Loss 0.7785 | Time(s) 5.6150 | Accuracy: 53.301983 %\n",
            "Epoch 00036 | Loss 0.8074 | Time(s) 5.6140 | Accuracy: 53.075139 %\n",
            "Epoch 00037 | Loss 0.7827 | Time(s) 5.6137 | Accuracy: 52.807514 %\n",
            "Epoch 00038 | Loss 0.9961 | Time(s) 5.6128 | Accuracy: 52.787123 %\n",
            "Epoch 00039 | Loss 0.8707 | Time(s) 5.6134 | Accuracy: 53.160524 %\n",
            "Epoch 00040 | Loss 0.7858 | Time(s) 5.6122 | Accuracy: 53.314727 %\n",
            "Epoch 00041 | Loss 0.8560 | Time(s) 5.6115 | Accuracy: 53.029260 %\n",
            "Epoch 00042 | Loss 0.7662 | Time(s) 5.6117 | Accuracy: 52.918387 %\n",
            "Epoch 00043 | Loss 0.8297 | Time(s) 5.6148 | Accuracy: 52.655860 %\n",
            "Epoch 00044 | Loss 0.7952 | Time(s) 5.6123 | Accuracy: 52.621451 %\n",
            "Epoch 00045 | Loss 0.9452 | Time(s) 5.6111 | Accuracy: 53.020340 %\n",
            "Epoch 00046 | Loss 0.8304 | Time(s) 5.6133 | Accuracy: 53.298160 %\n",
            "Epoch 00047 | Loss 0.7430 | Time(s) 5.6125 | Accuracy: 53.355508 %\n",
            "Epoch 00048 | Loss 0.7850 | Time(s) 5.6111 | Accuracy: 53.012693 %\n",
            "Epoch 00049 | Loss 0.7825 | Time(s) 5.6119 | Accuracy: 52.529694 %\n",
            "Epoch 00050 | Loss 0.8421 | Time(s) 5.6097 | Accuracy: 52.739970 %\n",
            "Epoch 00051 | Loss 0.7712 | Time(s) 5.6104 | Accuracy: 52.468522 %\n",
            "Epoch 00052 | Loss 0.7758 | Time(s) 5.6119 | Accuracy: 52.465973 %\n",
            "Epoch 00053 | Loss 0.7729 | Time(s) 5.6122 | Accuracy: 52.320691 %\n",
            "Epoch 00054 | Loss 0.8540 | Time(s) 5.6107 | Accuracy: 52.120610 %\n",
            "Epoch 00055 | Loss 0.8315 | Time(s) 5.6097 | Accuracy: 52.220013 %\n",
            "Epoch 00056 | Loss 0.7692 | Time(s) 5.6065 | Accuracy: 52.297752 %\n",
            "Epoch 00057 | Loss 0.8342 | Time(s) 5.6084 | Accuracy: 52.265892 %\n",
            "Epoch 00058 | Loss 0.7256 | Time(s) 5.6079 | Accuracy: 52.264617 %\n",
            "Epoch 00059 | Loss 0.8436 | Time(s) 5.6084 | Accuracy: 52.327063 %\n",
            "Epoch 00060 | Loss 0.8072 | Time(s) 5.6097 | Accuracy: 52.285008 %\n",
            "Epoch 00061 | Loss 0.7599 | Time(s) 5.6084 | Accuracy: 52.346179 %\n",
            "Epoch 00062 | Loss 0.8012 | Time(s) 5.6088 | Accuracy: 52.431564 %\n",
            "Epoch 00063 | Loss 0.7594 | Time(s) 5.6072 | Accuracy: 52.650762 %\n",
            "Epoch 00064 | Loss 0.7964 | Time(s) 5.6071 | Accuracy: 52.659683 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0708 | Time(s) 5.4454 | Accuracy: 23.415915 %\n",
            "Epoch 00001 | Loss 1.0214 | Time(s) 5.4830 | Accuracy: 31.807871 %\n",
            "Epoch 00002 | Loss 0.9819 | Time(s) 5.4659 | Accuracy: 42.500127 %\n",
            "Epoch 00003 | Loss 0.9193 | Time(s) 5.4691 | Accuracy: 57.000306 %\n",
            "Epoch 00004 | Loss 0.8735 | Time(s) 5.5055 | Accuracy: 69.158128 %\n",
            "Epoch 00005 | Loss 0.8305 | Time(s) 5.5292 | Accuracy: 74.782077 %\n",
            "Epoch 00006 | Loss 0.7638 | Time(s) 5.5525 | Accuracy: 75.916297 %\n",
            "Epoch 00007 | Loss 0.7869 | Time(s) 5.5406 | Accuracy: 75.484274 %\n",
            "Epoch 00008 | Loss 0.7360 | Time(s) 5.5454 | Accuracy: 75.940511 %\n",
            "Epoch 00009 | Loss 0.6733 | Time(s) 5.5438 | Accuracy: 79.405618 %\n",
            "Epoch 00010 | Loss 0.6703 | Time(s) 5.5560 | Accuracy: 79.083193 %\n",
            "Epoch 00011 | Loss 0.6359 | Time(s) 5.5528 | Accuracy: 80.485038 %\n",
            "Epoch 00012 | Loss 0.6086 | Time(s) 5.5540 | Accuracy: 80.915787 %\n",
            "Epoch 00013 | Loss 0.5984 | Time(s) 5.5552 | Accuracy: 80.262018 %\n",
            "Epoch 00014 | Loss 0.5858 | Time(s) 5.5622 | Accuracy: 81.066167 %\n",
            "Epoch 00015 | Loss 0.5795 | Time(s) 5.5657 | Accuracy: 80.933629 %\n",
            "Epoch 00016 | Loss 0.6290 | Time(s) 5.5683 | Accuracy: 81.484172 %\n",
            "Epoch 00017 | Loss 0.5295 | Time(s) 5.5627 | Accuracy: 81.629454 %\n",
            "Epoch 00018 | Loss 0.5510 | Time(s) 5.5623 | Accuracy: 80.403477 %\n",
            "Epoch 00019 | Loss 0.5006 | Time(s) 5.5640 | Accuracy: 81.137534 %\n",
            "Epoch 00020 | Loss 0.5097 | Time(s) 5.5628 | Accuracy: 81.945506 %\n",
            "Epoch 00021 | Loss 0.5350 | Time(s) 5.5672 | Accuracy: 81.876689 %\n",
            "Epoch 00022 | Loss 0.5089 | Time(s) 5.5654 | Accuracy: 81.779834 %\n",
            "Epoch 00023 | Loss 0.6776 | Time(s) 5.5686 | Accuracy: 80.978233 %\n",
            "Epoch 00024 | Loss 0.4830 | Time(s) 5.5716 | Accuracy: 81.406433 %\n",
            "Epoch 00025 | Loss 0.5552 | Time(s) 5.5711 | Accuracy: 81.559362 %\n",
            "Epoch 00026 | Loss 0.4655 | Time(s) 5.5746 | Accuracy: 81.633277 %\n",
            "Epoch 00027 | Loss 0.4984 | Time(s) 5.5806 | Accuracy: 81.569557 %\n",
            "Epoch 00028 | Loss 0.5562 | Time(s) 5.5831 | Accuracy: 81.452312 %\n",
            "Epoch 00029 | Loss 0.4974 | Time(s) 5.5844 | Accuracy: 81.556813 %\n",
            "Epoch 00030 | Loss 0.4542 | Time(s) 5.5869 | Accuracy: 81.861396 %\n",
            "Epoch 00031 | Loss 0.5081 | Time(s) 5.5902 | Accuracy: 81.798950 %\n",
            "Epoch 00032 | Loss 0.4979 | Time(s) 5.5899 | Accuracy: 81.653668 %\n",
            "Epoch 00033 | Loss 0.4623 | Time(s) 5.5909 | Accuracy: 81.711016 %\n",
            "Epoch 00034 | Loss 0.4309 | Time(s) 5.5909 | Accuracy: 81.726309 %\n",
            "Epoch 00035 | Loss 0.4143 | Time(s) 5.5923 | Accuracy: 81.856298 %\n",
            "Epoch 00036 | Loss 0.4339 | Time(s) 5.5908 | Accuracy: 81.690625 %\n",
            "Epoch 00037 | Loss 0.4508 | Time(s) 5.5889 | Accuracy: 81.233114 %\n",
            "Epoch 00038 | Loss 0.4960 | Time(s) 5.5924 | Accuracy: 81.537697 %\n",
            "Epoch 00039 | Loss 0.3939 | Time(s) 5.5917 | Accuracy: 81.851200 %\n",
            "Epoch 00040 | Loss 0.3824 | Time(s) 5.5939 | Accuracy: 82.032166 %\n",
            "Epoch 00041 | Loss 0.4697 | Time(s) 5.5948 | Accuracy: 81.652393 %\n",
            "Epoch 00042 | Loss 0.4570 | Time(s) 5.5973 | Accuracy: 81.834633 %\n",
            "Epoch 00043 | Loss 0.4099 | Time(s) 5.5981 | Accuracy: 81.653668 %\n",
            "Epoch 00044 | Loss 0.4699 | Time(s) 5.5968 | Accuracy: 81.335067 %\n",
            "Epoch 00045 | Loss 0.3912 | Time(s) 5.5975 | Accuracy: 81.654942 %\n",
            "Epoch 00046 | Loss 0.3996 | Time(s) 5.5976 | Accuracy: 81.495642 %\n",
            "Epoch 00047 | Loss 0.4227 | Time(s) 5.5977 | Accuracy: 81.394964 %\n",
            "Epoch 00048 | Loss 0.3700 | Time(s) 5.5992 | Accuracy: 81.528776 %\n",
            "Epoch 00049 | Loss 0.3915 | Time(s) 5.5984 | Accuracy: 80.933629 %\n",
            "Epoch 00050 | Loss 0.4065 | Time(s) 5.5969 | Accuracy: 81.568283 %\n",
            "Epoch 00051 | Loss 0.4025 | Time(s) 5.5987 | Accuracy: 81.895805 %\n",
            "Epoch 00052 | Loss 0.4066 | Time(s) 5.5981 | Accuracy: 81.927665 %\n",
            "Epoch 00053 | Loss 0.3825 | Time(s) 5.5981 | Accuracy: 81.855024 %\n",
            "Epoch 00054 | Loss 0.4126 | Time(s) 5.5978 | Accuracy: 81.930214 %\n",
            "Epoch 00055 | Loss 0.3981 | Time(s) 5.5970 | Accuracy: 81.949330 %\n",
            "Epoch 00056 | Loss 0.3924 | Time(s) 5.5966 | Accuracy: 81.884335 %\n",
            "Epoch 00057 | Loss 0.4087 | Time(s) 5.5964 | Accuracy: 81.833359 %\n",
            "Epoch 00058 | Loss 0.3983 | Time(s) 5.5957 | Accuracy: 81.598868 %\n",
            "Epoch 00059 | Loss 0.3437 | Time(s) 5.5978 | Accuracy: 81.828261 %\n",
            "Epoch 00060 | Loss 0.3831 | Time(s) 5.5995 | Accuracy: 81.875414 %\n",
            "Epoch 00061 | Loss 0.4119 | Time(s) 5.5979 | Accuracy: 81.907274 %\n",
            "Epoch 00062 | Loss 0.3773 | Time(s) 5.5978 | Accuracy: 81.705918 %\n",
            "Epoch 00063 | Loss 0.3700 | Time(s) 5.5969 | Accuracy: 81.749248 %\n",
            "Epoch 00064 | Loss 0.3448 | Time(s) 5.5964 | Accuracy: 81.856298 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0587 | Time(s) 5.4782 | Accuracy: 36.102615 %\n",
            "Epoch 00001 | Loss 0.9639 | Time(s) 5.5133 | Accuracy: 56.747974 %\n",
            "Epoch 00002 | Loss 0.9187 | Time(s) 5.5505 | Accuracy: 64.975531 %\n",
            "Epoch 00003 | Loss 0.8478 | Time(s) 5.6140 | Accuracy: 71.443136 %\n",
            "Epoch 00004 | Loss 0.8705 | Time(s) 5.6421 | Accuracy: 76.011877 %\n",
            "Epoch 00005 | Loss 0.7642 | Time(s) 5.6284 | Accuracy: 76.515267 %\n",
            "Epoch 00006 | Loss 0.7323 | Time(s) 5.6281 | Accuracy: 76.924351 %\n",
            "Epoch 00007 | Loss 0.7063 | Time(s) 5.6194 | Accuracy: 77.745068 %\n",
            "Epoch 00008 | Loss 0.6864 | Time(s) 5.6177 | Accuracy: 78.897130 %\n",
            "Epoch 00009 | Loss 0.6763 | Time(s) 5.6076 | Accuracy: 79.733140 %\n",
            "Epoch 00010 | Loss 0.6346 | Time(s) 5.6046 | Accuracy: 80.790896 %\n",
            "Epoch 00011 | Loss 0.6106 | Time(s) 5.6048 | Accuracy: 80.580619 %\n",
            "Epoch 00012 | Loss 0.6360 | Time(s) 5.6008 | Accuracy: 80.465922 %\n",
            "Epoch 00013 | Loss 0.5922 | Time(s) 5.6024 | Accuracy: 79.906459 %\n",
            "Epoch 00014 | Loss 0.6068 | Time(s) 5.6097 | Accuracy: 80.163888 %\n",
            "Epoch 00015 | Loss 0.5885 | Time(s) 5.6074 | Accuracy: 80.901769 %\n",
            "Epoch 00016 | Loss 0.5981 | Time(s) 5.6056 | Accuracy: 80.683846 %\n",
            "Epoch 00017 | Loss 0.5195 | Time(s) 5.6045 | Accuracy: 80.657083 %\n",
            "Epoch 00018 | Loss 0.5253 | Time(s) 5.6057 | Accuracy: 80.524545 %\n",
            "Epoch 00019 | Loss 0.5432 | Time(s) 5.6008 | Accuracy: 80.583168 %\n",
            "Epoch 00020 | Loss 0.5468 | Time(s) 5.5979 | Accuracy: 80.872458 %\n",
            "Epoch 00021 | Loss 0.5084 | Time(s) 5.5949 | Accuracy: 81.245858 %\n",
            "Epoch 00022 | Loss 0.5080 | Time(s) 5.5965 | Accuracy: 81.732681 %\n",
            "Epoch 00023 | Loss 0.5504 | Time(s) 5.6020 | Accuracy: 82.028343 %\n",
            "Epoch 00024 | Loss 0.5129 | Time(s) 5.5956 | Accuracy: 81.974818 %\n",
            "Epoch 00025 | Loss 0.4833 | Time(s) 5.5927 | Accuracy: 81.985013 %\n",
            "Epoch 00026 | Loss 0.5157 | Time(s) 5.5932 | Accuracy: 80.933629 %\n",
            "Epoch 00027 | Loss 0.4942 | Time(s) 5.5896 | Accuracy: 80.343580 %\n",
            "Epoch 00028 | Loss 0.4624 | Time(s) 5.5909 | Accuracy: 81.414080 %\n",
            "Epoch 00029 | Loss 0.4734 | Time(s) 5.5913 | Accuracy: 81.725034 %\n",
            "Epoch 00030 | Loss 0.4191 | Time(s) 5.5878 | Accuracy: 81.744150 %\n",
            "Epoch 00031 | Loss 0.4541 | Time(s) 5.5849 | Accuracy: 81.691900 %\n",
            "Epoch 00032 | Loss 0.5916 | Time(s) 5.5789 | Accuracy: 81.681705 %\n",
            "Epoch 00033 | Loss 0.4758 | Time(s) 5.5767 | Accuracy: 82.115002 %\n",
            "Epoch 00034 | Loss 0.4575 | Time(s) 5.5777 | Accuracy: 82.062752 %\n",
            "Epoch 00035 | Loss 0.4325 | Time(s) 5.5759 | Accuracy: 81.953153 %\n",
            "Epoch 00036 | Loss 0.4614 | Time(s) 5.5739 | Accuracy: 81.307030 %\n",
            "Epoch 00037 | Loss 0.4487 | Time(s) 5.5731 | Accuracy: 81.535148 %\n",
            "Epoch 00038 | Loss 0.4440 | Time(s) 5.5694 | Accuracy: 81.923842 %\n",
            "Epoch 00039 | Loss 0.4404 | Time(s) 5.5688 | Accuracy: 81.973543 %\n",
            "Epoch 00040 | Loss 0.4239 | Time(s) 5.5679 | Accuracy: 81.625631 %\n",
            "Epoch 00041 | Loss 0.4030 | Time(s) 5.5667 | Accuracy: 81.417903 %\n",
            "Epoch 00042 | Loss 0.3987 | Time(s) 5.5625 | Accuracy: 82.051282 %\n",
            "Epoch 00043 | Loss 0.4563 | Time(s) 5.5581 | Accuracy: 82.304889 %\n",
            "Epoch 00044 | Loss 0.3775 | Time(s) 5.5554 | Accuracy: 82.267931 %\n",
            "Epoch 00045 | Loss 0.4147 | Time(s) 5.5566 | Accuracy: 81.538971 %\n",
            "Epoch 00046 | Loss 0.4058 | Time(s) 5.5553 | Accuracy: 81.115869 %\n",
            "Epoch 00047 | Loss 0.3824 | Time(s) 5.5540 | Accuracy: 82.115002 %\n",
            "Epoch 00048 | Loss 0.4221 | Time(s) 5.5560 | Accuracy: 82.281949 %\n",
            "Epoch 00049 | Loss 0.3814 | Time(s) 5.5561 | Accuracy: 82.173625 %\n",
            "Epoch 00050 | Loss 0.3609 | Time(s) 5.5536 | Accuracy: 82.204211 %\n",
            "Epoch 00051 | Loss 0.4253 | Time(s) 5.5522 | Accuracy: 82.007952 %\n",
            "Epoch 00052 | Loss 0.3632 | Time(s) 5.5499 | Accuracy: 81.750523 %\n",
            "Epoch 00053 | Loss 0.3956 | Time(s) 5.5486 | Accuracy: 81.555539 %\n",
            "Epoch 00054 | Loss 0.3587 | Time(s) 5.5474 | Accuracy: 81.945506 %\n",
            "Epoch 00055 | Loss 0.4079 | Time(s) 5.5458 | Accuracy: 82.267931 %\n",
            "Epoch 00056 | Loss 0.4027 | Time(s) 5.5447 | Accuracy: 82.325279 %\n",
            "Epoch 00057 | Loss 0.3994 | Time(s) 5.5419 | Accuracy: 81.946781 %\n",
            "Epoch 00058 | Loss 0.4095 | Time(s) 5.5421 | Accuracy: 82.088240 %\n",
            "Epoch 00059 | Loss 0.3598 | Time(s) 5.5443 | Accuracy: 81.940409 %\n",
            "Epoch 00060 | Loss 0.3551 | Time(s) 5.5439 | Accuracy: 81.661314 %\n",
            "Epoch 00061 | Loss 0.3538 | Time(s) 5.5438 | Accuracy: 81.581027 %\n",
            "Epoch 00062 | Loss 0.3571 | Time(s) 5.5435 | Accuracy: 81.527502 %\n",
            "Epoch 00063 | Loss 0.4009 | Time(s) 5.5414 | Accuracy: 81.675333 %\n",
            "Epoch 00064 | Loss 0.3402 | Time(s) 5.5390 | Accuracy: 81.531325 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0567 | Time(s) 5.3598 | Accuracy: 24.793546 %\n",
            "Epoch 00001 | Loss 1.0201 | Time(s) 5.3563 | Accuracy: 33.130703 %\n",
            "Epoch 00002 | Loss 0.9634 | Time(s) 5.4009 | Accuracy: 36.924606 %\n",
            "Epoch 00003 | Loss 1.0318 | Time(s) 5.4053 | Accuracy: 39.760157 %\n",
            "Epoch 00004 | Loss 0.9272 | Time(s) 5.4029 | Accuracy: 42.168782 %\n",
            "Epoch 00005 | Loss 0.9297 | Time(s) 5.3915 | Accuracy: 45.563797 %\n",
            "Epoch 00006 | Loss 0.9008 | Time(s) 5.3938 | Accuracy: 48.045063 %\n",
            "Epoch 00007 | Loss 0.8442 | Time(s) 5.4073 | Accuracy: 54.619718 %\n",
            "Epoch 00008 | Loss 0.8166 | Time(s) 5.4054 | Accuracy: 65.361676 %\n",
            "Epoch 00009 | Loss 0.7623 | Time(s) 5.4145 | Accuracy: 73.053984 %\n",
            "Epoch 00010 | Loss 0.7014 | Time(s) 5.4060 | Accuracy: 77.262069 %\n",
            "Epoch 00011 | Loss 0.6703 | Time(s) 5.4043 | Accuracy: 78.573431 %\n",
            "Epoch 00012 | Loss 0.6468 | Time(s) 5.4091 | Accuracy: 79.154560 %\n",
            "Epoch 00013 | Loss 0.6124 | Time(s) 5.4145 | Accuracy: 80.061936 %\n",
            "Epoch 00014 | Loss 0.5671 | Time(s) 5.4125 | Accuracy: 80.755212 %\n",
            "Epoch 00015 | Loss 0.6749 | Time(s) 5.4281 | Accuracy: 80.894122 %\n",
            "Epoch 00016 | Loss 0.5241 | Time(s) 5.4261 | Accuracy: 81.008819 %\n",
            "Epoch 00017 | Loss 0.5571 | Time(s) 5.4242 | Accuracy: 80.965489 %\n",
            "Epoch 00018 | Loss 0.5502 | Time(s) 5.4303 | Accuracy: 81.155375 %\n",
            "Epoch 00019 | Loss 0.5389 | Time(s) 5.4264 | Accuracy: 81.199980 %\n",
            "Epoch 00020 | Loss 0.5218 | Time(s) 5.4248 | Accuracy: 81.884335 %\n",
            "Epoch 00021 | Loss 0.5891 | Time(s) 5.4233 | Accuracy: 81.993934 %\n",
            "Epoch 00022 | Loss 0.5303 | Time(s) 5.4240 | Accuracy: 81.942958 %\n",
            "Epoch 00023 | Loss 0.4720 | Time(s) 5.4255 | Accuracy: 81.609064 %\n",
            "Epoch 00024 | Loss 0.4621 | Time(s) 5.4305 | Accuracy: 81.837182 %\n",
            "Epoch 00025 | Loss 0.6076 | Time(s) 5.4306 | Accuracy: 81.903451 %\n",
            "Epoch 00026 | Loss 0.5005 | Time(s) 5.4381 | Accuracy: 82.016873 %\n",
            "Epoch 00027 | Loss 0.4524 | Time(s) 5.4371 | Accuracy: 82.094612 %\n",
            "Epoch 00028 | Loss 0.4528 | Time(s) 5.4389 | Accuracy: 81.861396 %\n",
            "Epoch 00029 | Loss 0.4685 | Time(s) 5.4442 | Accuracy: 81.745425 %\n",
            "Epoch 00030 | Loss 0.4284 | Time(s) 5.4477 | Accuracy: 82.115002 %\n",
            "Epoch 00031 | Loss 0.4686 | Time(s) 5.4479 | Accuracy: 82.174899 %\n",
            "Epoch 00032 | Loss 0.4311 | Time(s) 5.4488 | Accuracy: 82.158332 %\n",
            "Epoch 00033 | Loss 0.6286 | Time(s) 5.4468 | Accuracy: 81.498190 %\n",
            "Epoch 00034 | Loss 0.4186 | Time(s) 5.4474 | Accuracy: 81.866493 %\n",
            "Epoch 00035 | Loss 0.4491 | Time(s) 5.4469 | Accuracy: 82.227150 %\n",
            "Epoch 00036 | Loss 0.4486 | Time(s) 5.4486 | Accuracy: 82.125198 %\n",
            "Epoch 00037 | Loss 0.3968 | Time(s) 5.4508 | Accuracy: 82.108630 %\n",
            "Epoch 00038 | Loss 0.4266 | Time(s) 5.4510 | Accuracy: 82.075496 %\n",
            "Epoch 00039 | Loss 0.4429 | Time(s) 5.4519 | Accuracy: 81.790029 %\n",
            "Epoch 00040 | Loss 0.4222 | Time(s) 5.4553 | Accuracy: 81.855024 %\n",
            "Epoch 00041 | Loss 0.4095 | Time(s) 5.4554 | Accuracy: 82.092063 %\n",
            "Epoch 00042 | Loss 0.4416 | Time(s) 5.4586 | Accuracy: 82.035989 %\n",
            "Epoch 00043 | Loss 0.4657 | Time(s) 5.4568 | Accuracy: 82.046184 %\n",
            "Epoch 00044 | Loss 0.3998 | Time(s) 5.4563 | Accuracy: 82.163430 %\n",
            "Epoch 00045 | Loss 0.4300 | Time(s) 5.4566 | Accuracy: 82.041087 %\n",
            "Epoch 00046 | Loss 0.4191 | Time(s) 5.4585 | Accuracy: 81.891981 %\n",
            "Epoch 00047 | Loss 0.4073 | Time(s) 5.4585 | Accuracy: 81.983739 %\n",
            "Epoch 00048 | Loss 0.3734 | Time(s) 5.4598 | Accuracy: 82.174899 %\n",
            "Epoch 00049 | Loss 0.4128 | Time(s) 5.4593 | Accuracy: 82.285773 %\n",
            "Epoch 00050 | Loss 0.3623 | Time(s) 5.4607 | Accuracy: 82.420859 %\n",
            "Epoch 00051 | Loss 0.3924 | Time(s) 5.4613 | Accuracy: 82.219503 %\n",
            "Epoch 00052 | Loss 0.3527 | Time(s) 5.4618 | Accuracy: 82.164704 %\n",
            "Epoch 00053 | Loss 0.3854 | Time(s) 5.4624 | Accuracy: 82.084417 %\n",
            "Epoch 00054 | Loss 0.3758 | Time(s) 5.4629 | Accuracy: 81.718662 %\n",
            "Epoch 00055 | Loss 0.3849 | Time(s) 5.4631 | Accuracy: 80.989703 %\n",
            "Epoch 00056 | Loss 0.3552 | Time(s) 5.4650 | Accuracy: 81.349085 %\n",
            "Epoch 00057 | Loss 0.4052 | Time(s) 5.4650 | Accuracy: 82.004129 %\n",
            "Epoch 00058 | Loss 0.3347 | Time(s) 5.4639 | Accuracy: 82.016873 %\n",
            "Epoch 00059 | Loss 0.3318 | Time(s) 5.4668 | Accuracy: 82.010501 %\n",
            "Epoch 00060 | Loss 0.3681 | Time(s) 5.4665 | Accuracy: 81.995208 %\n",
            "Epoch 00061 | Loss 0.3238 | Time(s) 5.4676 | Accuracy: 82.019422 %\n",
            "Epoch 00062 | Loss 0.3479 | Time(s) 5.4672 | Accuracy: 81.935311 %\n",
            "Epoch 00063 | Loss 0.3573 | Time(s) 5.4677 | Accuracy: 82.121374 %\n",
            "Epoch 00064 | Loss 0.3549 | Time(s) 5.4680 | Accuracy: 81.969720 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0451 | Time(s) 5.4387 | Accuracy: 40.624203 %\n",
            "Epoch 00001 | Loss 1.0025 | Time(s) 5.4539 | Accuracy: 48.282102 %\n",
            "Epoch 00002 | Loss 0.9429 | Time(s) 5.4934 | Accuracy: 59.800173 %\n",
            "Epoch 00003 | Loss 0.8531 | Time(s) 5.5350 | Accuracy: 64.525667 %\n",
            "Epoch 00004 | Loss 0.8328 | Time(s) 5.5562 | Accuracy: 67.830198 %\n",
            "Epoch 00005 | Loss 0.7747 | Time(s) 5.5716 | Accuracy: 70.436356 %\n",
            "Epoch 00006 | Loss 0.7588 | Time(s) 5.5898 | Accuracy: 72.423153 %\n",
            "Epoch 00007 | Loss 0.7433 | Time(s) 5.6049 | Accuracy: 74.074782 %\n",
            "Epoch 00008 | Loss 0.7008 | Time(s) 5.6097 | Accuracy: 74.766784 %\n",
            "Epoch 00009 | Loss 0.8055 | Time(s) 5.6227 | Accuracy: 75.997859 %\n",
            "Epoch 00010 | Loss 0.6737 | Time(s) 5.6334 | Accuracy: 77.431564 %\n",
            "Epoch 00011 | Loss 0.7575 | Time(s) 5.6373 | Accuracy: 77.937503 %\n",
            "Epoch 00012 | Loss 0.7321 | Time(s) 5.6461 | Accuracy: 78.129938 %\n",
            "Epoch 00013 | Loss 0.5934 | Time(s) 5.6506 | Accuracy: 78.800275 %\n",
            "Epoch 00014 | Loss 0.5940 | Time(s) 5.6526 | Accuracy: 79.752256 %\n",
            "Epoch 00015 | Loss 0.5514 | Time(s) 5.6628 | Accuracy: 80.493959 %\n",
            "Epoch 00016 | Loss 0.5361 | Time(s) 5.6640 | Accuracy: 80.729724 %\n",
            "Epoch 00017 | Loss 0.5237 | Time(s) 5.6699 | Accuracy: 80.900494 %\n",
            "Epoch 00018 | Loss 0.5151 | Time(s) 5.6809 | Accuracy: 81.254779 %\n",
            "Epoch 00019 | Loss 0.6905 | Time(s) 5.6828 | Accuracy: 81.468879 %\n",
            "Epoch 00020 | Loss 0.5668 | Time(s) 5.6870 | Accuracy: 81.638375 %\n",
            "Epoch 00021 | Loss 0.5388 | Time(s) 5.6937 | Accuracy: 81.335067 %\n",
            "Epoch 00022 | Loss 0.4800 | Time(s) 5.6988 | Accuracy: 81.031758 %\n",
            "Epoch 00023 | Loss 0.5933 | Time(s) 5.7045 | Accuracy: 81.612887 %\n",
            "Epoch 00024 | Loss 0.4921 | Time(s) 5.7090 | Accuracy: 81.709742 %\n",
            "Epoch 00025 | Loss 0.5526 | Time(s) 5.7091 | Accuracy: 81.272621 %\n",
            "Epoch 00026 | Loss 0.4989 | Time(s) 5.7129 | Accuracy: 81.337615 %\n",
            "Epoch 00027 | Loss 0.4790 | Time(s) 5.7136 | Accuracy: 81.668961 %\n",
            "Epoch 00028 | Loss 0.4543 | Time(s) 5.7204 | Accuracy: 81.632003 %\n",
            "Epoch 00029 | Loss 0.5952 | Time(s) 5.7240 | Accuracy: 81.876689 %\n",
            "Epoch 00030 | Loss 0.4906 | Time(s) 5.7258 | Accuracy: 81.712290 %\n",
            "Epoch 00031 | Loss 0.4569 | Time(s) 5.7254 | Accuracy: 81.914921 %\n",
            "Epoch 00032 | Loss 0.4328 | Time(s) 5.7261 | Accuracy: 81.988836 %\n",
            "Epoch 00033 | Loss 0.5234 | Time(s) 5.7275 | Accuracy: 81.871591 %\n",
            "Epoch 00034 | Loss 0.4641 | Time(s) 5.7274 | Accuracy: 81.795127 %\n",
            "Epoch 00035 | Loss 0.4459 | Time(s) 5.7272 | Accuracy: 82.066575 %\n",
            "Epoch 00036 | Loss 0.4249 | Time(s) 5.7271 | Accuracy: 81.828261 %\n",
            "Epoch 00037 | Loss 0.4191 | Time(s) 5.7272 | Accuracy: 81.923842 %\n",
            "Epoch 00038 | Loss 0.4162 | Time(s) 5.7300 | Accuracy: 81.774736 %\n",
            "Epoch 00039 | Loss 0.4727 | Time(s) 5.7323 | Accuracy: 81.912372 %\n",
            "Epoch 00040 | Loss 0.4650 | Time(s) 5.7362 | Accuracy: 81.956976 %\n",
            "Epoch 00041 | Loss 0.4024 | Time(s) 5.7377 | Accuracy: 81.602692 %\n",
            "Epoch 00042 | Loss 0.3811 | Time(s) 5.7381 | Accuracy: 81.466330 %\n",
            "Epoch 00043 | Loss 0.3887 | Time(s) 5.7383 | Accuracy: 81.704644 %\n",
            "Epoch 00044 | Loss 0.4126 | Time(s) 5.7398 | Accuracy: 81.581027 %\n",
            "Epoch 00045 | Loss 0.3851 | Time(s) 5.7429 | Accuracy: 81.686802 %\n",
            "Epoch 00046 | Loss 0.3932 | Time(s) 5.7442 | Accuracy: 81.653668 %\n",
            "Epoch 00047 | Loss 0.3948 | Time(s) 5.7445 | Accuracy: 81.732681 %\n",
            "Epoch 00048 | Loss 0.3668 | Time(s) 5.7440 | Accuracy: 81.842280 %\n",
            "Epoch 00049 | Loss 0.4092 | Time(s) 5.7477 | Accuracy: 81.863945 %\n",
            "Epoch 00050 | Loss 0.4139 | Time(s) 5.7478 | Accuracy: 81.821889 %\n",
            "Epoch 00051 | Loss 0.3652 | Time(s) 5.7476 | Accuracy: 81.770913 %\n",
            "Epoch 00052 | Loss 0.4051 | Time(s) 5.7471 | Accuracy: 81.742876 %\n",
            "Epoch 00053 | Loss 0.3608 | Time(s) 5.7486 | Accuracy: 81.810420 %\n",
            "Epoch 00054 | Loss 0.4050 | Time(s) 5.7504 | Accuracy: 81.843554 %\n",
            "Epoch 00055 | Loss 0.3756 | Time(s) 5.7526 | Accuracy: 81.877963 %\n",
            "Epoch 00056 | Loss 0.4137 | Time(s) 5.7525 | Accuracy: 81.753071 %\n",
            "Epoch 00057 | Loss 0.3643 | Time(s) 5.7538 | Accuracy: 81.649845 %\n",
            "Epoch 00058 | Loss 0.3962 | Time(s) 5.7545 | Accuracy: 81.704644 %\n",
            "Epoch 00059 | Loss 0.3311 | Time(s) 5.7561 | Accuracy: 81.765815 %\n",
            "Epoch 00060 | Loss 0.3878 | Time(s) 5.7577 | Accuracy: 81.751797 %\n",
            "Epoch 00061 | Loss 0.3475 | Time(s) 5.7591 | Accuracy: 81.816792 %\n",
            "Epoch 00062 | Loss 0.3850 | Time(s) 5.7596 | Accuracy: 81.852475 %\n",
            "Epoch 00063 | Loss 0.3355 | Time(s) 5.7614 | Accuracy: 81.814243 %\n",
            "Epoch 00064 | Loss 0.3253 | Time(s) 5.7633 | Accuracy: 81.870317 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0252 | Time(s) 5.8590 | Accuracy: 40.752918 %\n",
            "Epoch 00001 | Loss 1.0219 | Time(s) 5.8547 | Accuracy: 42.609726 %\n",
            "Epoch 00002 | Loss 1.0194 | Time(s) 5.8415 | Accuracy: 44.446144 %\n",
            "Epoch 00003 | Loss 0.9833 | Time(s) 5.8511 | Accuracy: 46.343732 %\n",
            "Epoch 00004 | Loss 0.9079 | Time(s) 5.8804 | Accuracy: 54.998216 %\n",
            "Epoch 00005 | Loss 0.9621 | Time(s) 5.8864 | Accuracy: 59.160422 %\n",
            "Epoch 00006 | Loss 0.7978 | Time(s) 5.9020 | Accuracy: 63.392721 %\n",
            "Epoch 00007 | Loss 0.8861 | Time(s) 5.8935 | Accuracy: 69.286843 %\n",
            "Epoch 00008 | Loss 0.8705 | Time(s) 5.8973 | Accuracy: 71.092675 %\n",
            "Epoch 00009 | Loss 0.7735 | Time(s) 5.8897 | Accuracy: 71.203548 %\n",
            "Epoch 00010 | Loss 0.6800 | Time(s) 5.8944 | Accuracy: 72.729011 %\n",
            "Epoch 00011 | Loss 0.6910 | Time(s) 5.8981 | Accuracy: 72.771066 %\n",
            "Epoch 00012 | Loss 0.7768 | Time(s) 5.8863 | Accuracy: 74.942652 %\n",
            "Epoch 00013 | Loss 0.7594 | Time(s) 5.8834 | Accuracy: 77.128256 %\n",
            "Epoch 00014 | Loss 0.8380 | Time(s) 5.8814 | Accuracy: 77.739970 %\n",
            "Epoch 00015 | Loss 0.7188 | Time(s) 5.8824 | Accuracy: 77.309222 %\n",
            "Epoch 00016 | Loss 0.6306 | Time(s) 5.8804 | Accuracy: 77.128256 %\n",
            "Epoch 00017 | Loss 0.6217 | Time(s) 5.8831 | Accuracy: 76.847887 %\n",
            "Epoch 00018 | Loss 0.7198 | Time(s) 5.8838 | Accuracy: 76.384004 %\n",
            "Epoch 00019 | Loss 0.6089 | Time(s) 5.8813 | Accuracy: 76.526737 %\n",
            "Epoch 00020 | Loss 0.6577 | Time(s) 5.8807 | Accuracy: 77.523322 %\n",
            "Epoch 00021 | Loss 0.6221 | Time(s) 5.8804 | Accuracy: 77.917113 %\n",
            "Epoch 00022 | Loss 0.6740 | Time(s) 5.8789 | Accuracy: 78.291788 %\n",
            "Epoch 00023 | Loss 0.7505 | Time(s) 5.8770 | Accuracy: 78.488046 %\n",
            "Epoch 00024 | Loss 0.6309 | Time(s) 5.8765 | Accuracy: 78.225519 %\n",
            "Epoch 00025 | Loss 0.6774 | Time(s) 5.8754 | Accuracy: 77.414997 %\n",
            "Epoch 00026 | Loss 0.5715 | Time(s) 5.8763 | Accuracy: 76.371260 %\n",
            "Epoch 00027 | Loss 0.6352 | Time(s) 5.8748 | Accuracy: 76.066677 %\n",
            "Epoch 00028 | Loss 0.5706 | Time(s) 5.8726 | Accuracy: 77.697915 %\n",
            "Epoch 00029 | Loss 0.5956 | Time(s) 5.8729 | Accuracy: 78.838507 %\n",
            "Epoch 00030 | Loss 0.6087 | Time(s) 5.8704 | Accuracy: 78.906051 %\n",
            "Epoch 00031 | Loss 0.5833 | Time(s) 5.8697 | Accuracy: 77.940052 %\n",
            "Epoch 00032 | Loss 0.6091 | Time(s) 5.8678 | Accuracy: 77.771831 %\n",
            "Epoch 00033 | Loss 0.6081 | Time(s) 5.8669 | Accuracy: 78.171994 %\n",
            "Epoch 00034 | Loss 0.5696 | Time(s) 5.8682 | Accuracy: 78.468930 %\n",
            "Epoch 00035 | Loss 0.5450 | Time(s) 5.8687 | Accuracy: 78.878014 %\n",
            "Epoch 00036 | Loss 0.5758 | Time(s) 5.8702 | Accuracy: 79.132895 %\n",
            "Epoch 00037 | Loss 0.5419 | Time(s) 5.8705 | Accuracy: 79.202987 %\n",
            "Epoch 00038 | Loss 0.5742 | Time(s) 5.8723 | Accuracy: 78.732732 %\n",
            "Epoch 00039 | Loss 0.5683 | Time(s) 5.8702 | Accuracy: 78.981241 %\n",
            "Epoch 00040 | Loss 0.5688 | Time(s) 5.8707 | Accuracy: 78.886935 %\n",
            "Epoch 00041 | Loss 0.5260 | Time(s) 5.8713 | Accuracy: 78.835959 %\n",
            "Epoch 00042 | Loss 0.7025 | Time(s) 5.8703 | Accuracy: 78.811745 %\n",
            "Epoch 00043 | Loss 0.5460 | Time(s) 5.8708 | Accuracy: 78.935362 %\n",
            "Epoch 00044 | Loss 0.5618 | Time(s) 5.8684 | Accuracy: 79.200438 %\n",
            "Epoch 00045 | Loss 0.5172 | Time(s) 5.8708 | Accuracy: 78.920069 %\n",
            "Epoch 00046 | Loss 0.5392 | Time(s) 5.8702 | Accuracy: 78.921344 %\n",
            "Epoch 00047 | Loss 0.5473 | Time(s) 5.8706 | Accuracy: 78.827038 %\n",
            "Epoch 00048 | Loss 0.4923 | Time(s) 5.8691 | Accuracy: 78.665188 %\n",
            "Epoch 00049 | Loss 0.5127 | Time(s) 5.8675 | Accuracy: 78.508437 %\n",
            "Epoch 00050 | Loss 0.5067 | Time(s) 5.8678 | Accuracy: 78.398838 %\n",
            "Epoch 00051 | Loss 0.5352 | Time(s) 5.8668 | Accuracy: 78.731457 %\n",
            "Epoch 00052 | Loss 0.5369 | Time(s) 5.8678 | Accuracy: 78.976143 %\n",
            "Epoch 00053 | Loss 0.5794 | Time(s) 5.8671 | Accuracy: 78.551766 %\n",
            "Epoch 00054 | Loss 0.5939 | Time(s) 5.8671 | Accuracy: 78.907325 %\n",
            "Epoch 00055 | Loss 0.5356 | Time(s) 5.8679 | Accuracy: 78.709793 %\n",
            "Epoch 00056 | Loss 0.5371 | Time(s) 5.8681 | Accuracy: 78.774787 %\n",
            "Epoch 00057 | Loss 0.5582 | Time(s) 5.8674 | Accuracy: 78.781159 %\n",
            "Epoch 00058 | Loss 0.5295 | Time(s) 5.8662 | Accuracy: 78.582352 %\n",
            "Epoch 00059 | Loss 0.5227 | Time(s) 5.8649 | Accuracy: 78.774787 %\n",
            "Epoch 00060 | Loss 0.5338 | Time(s) 5.8638 | Accuracy: 78.828312 %\n",
            "Epoch 00061 | Loss 0.4839 | Time(s) 5.8634 | Accuracy: 78.528827 %\n",
            "Epoch 00062 | Loss 0.5214 | Time(s) 5.8635 | Accuracy: 78.401387 %\n",
            "Epoch 00063 | Loss 0.5559 | Time(s) 5.8645 | Accuracy: 78.689402 %\n",
            "Epoch 00064 | Loss 0.5503 | Time(s) 5.8641 | Accuracy: 78.912423 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0768 | Time(s) 5.8376 | Accuracy: 38.632309 %\n",
            "Epoch 00001 | Loss 1.0166 | Time(s) 5.8654 | Accuracy: 44.434674 %\n",
            "Epoch 00002 | Loss 0.9640 | Time(s) 5.8821 | Accuracy: 51.254014 %\n",
            "Epoch 00003 | Loss 0.8987 | Time(s) 5.8654 | Accuracy: 56.923842 %\n",
            "Epoch 00004 | Loss 0.8416 | Time(s) 5.8415 | Accuracy: 67.635214 %\n",
            "Epoch 00005 | Loss 0.7665 | Time(s) 5.8401 | Accuracy: 73.898914 %\n",
            "Epoch 00006 | Loss 0.8004 | Time(s) 5.8380 | Accuracy: 75.124892 %\n",
            "Epoch 00007 | Loss 0.7845 | Time(s) 5.8376 | Accuracy: 76.498700 %\n",
            "Epoch 00008 | Loss 0.6688 | Time(s) 5.8332 | Accuracy: 77.256971 %\n",
            "Epoch 00009 | Loss 0.6432 | Time(s) 5.8218 | Accuracy: 78.101901 %\n",
            "Epoch 00010 | Loss 0.6800 | Time(s) 5.8257 | Accuracy: 79.343172 %\n",
            "Epoch 00011 | Loss 0.6344 | Time(s) 5.8284 | Accuracy: 79.115053 %\n",
            "Epoch 00012 | Loss 0.5471 | Time(s) 5.8245 | Accuracy: 78.030535 %\n",
            "Epoch 00013 | Loss 0.5678 | Time(s) 5.8230 | Accuracy: 79.248866 %\n",
            "Epoch 00014 | Loss 0.5584 | Time(s) 5.8196 | Accuracy: 80.863537 %\n",
            "Epoch 00015 | Loss 0.5418 | Time(s) 5.8252 | Accuracy: 81.006270 %\n",
            "Epoch 00016 | Loss 0.5501 | Time(s) 5.8265 | Accuracy: 80.139675 %\n",
            "Epoch 00017 | Loss 0.4759 | Time(s) 5.8234 | Accuracy: 79.832543 %\n",
            "Epoch 00018 | Loss 0.5437 | Time(s) 5.8239 | Accuracy: 79.512668 %\n",
            "Epoch 00019 | Loss 0.5411 | Time(s) 5.8266 | Accuracy: 79.982923 %\n",
            "Epoch 00020 | Loss 0.6173 | Time(s) 5.8229 | Accuracy: 81.605240 %\n",
            "Epoch 00021 | Loss 0.4762 | Time(s) 5.8310 | Accuracy: 82.009227 %\n",
            "Epoch 00022 | Loss 0.5230 | Time(s) 5.8271 | Accuracy: 81.731406 %\n",
            "Epoch 00023 | Loss 0.4780 | Time(s) 5.8279 | Accuracy: 80.051741 %\n",
            "Epoch 00024 | Loss 0.4947 | Time(s) 5.8265 | Accuracy: 79.442575 %\n",
            "Epoch 00025 | Loss 0.5347 | Time(s) 5.8281 | Accuracy: 81.672784 %\n",
            "Epoch 00026 | Loss 0.5294 | Time(s) 5.8292 | Accuracy: 81.930214 %\n",
            "Epoch 00027 | Loss 0.5361 | Time(s) 5.8270 | Accuracy: 82.028343 %\n",
            "Epoch 00028 | Loss 0.4393 | Time(s) 5.8264 | Accuracy: 81.807871 %\n",
            "Epoch 00029 | Loss 0.4531 | Time(s) 5.8276 | Accuracy: 81.423000 %\n",
            "Epoch 00030 | Loss 0.4576 | Time(s) 5.8268 | Accuracy: 81.272621 %\n",
            "Epoch 00031 | Loss 0.4232 | Time(s) 5.8296 | Accuracy: 81.351634 %\n",
            "Epoch 00032 | Loss 0.4295 | Time(s) 5.8298 | Accuracy: 81.524953 %\n",
            "Epoch 00033 | Loss 0.4562 | Time(s) 5.8331 | Accuracy: 81.301932 %\n",
            "Epoch 00034 | Loss 0.4160 | Time(s) 5.8304 | Accuracy: 81.584850 %\n",
            "Epoch 00035 | Loss 0.4240 | Time(s) 5.8263 | Accuracy: 81.663863 %\n",
            "Epoch 00036 | Loss 0.3997 | Time(s) 5.8256 | Accuracy: 81.482897 %\n",
            "Epoch 00037 | Loss 0.4257 | Time(s) 5.8238 | Accuracy: 81.753071 %\n",
            "Epoch 00038 | Loss 0.4004 | Time(s) 5.8232 | Accuracy: 81.796401 %\n",
            "Epoch 00039 | Loss 0.4209 | Time(s) 5.8244 | Accuracy: 81.735230 %\n",
            "Epoch 00040 | Loss 0.4020 | Time(s) 5.8228 | Accuracy: 81.783657 %\n",
            "Epoch 00041 | Loss 0.4683 | Time(s) 5.8219 | Accuracy: 81.750523 %\n",
            "Epoch 00042 | Loss 0.4193 | Time(s) 5.8234 | Accuracy: 81.484172 %\n",
            "Epoch 00043 | Loss 0.4155 | Time(s) 5.8231 | Accuracy: 81.444665 %\n",
            "Epoch 00044 | Loss 0.4035 | Time(s) 5.8231 | Accuracy: 81.577203 %\n",
            "Epoch 00045 | Loss 0.3647 | Time(s) 5.8231 | Accuracy: 81.522404 %\n",
            "Epoch 00046 | Loss 0.4048 | Time(s) 5.8222 | Accuracy: 81.193608 %\n",
            "Epoch 00047 | Loss 0.4097 | Time(s) 5.8213 | Accuracy: 81.147729 %\n",
            "Epoch 00048 | Loss 0.4286 | Time(s) 5.8212 | Accuracy: 81.465056 %\n",
            "Epoch 00049 | Loss 0.3691 | Time(s) 5.8216 | Accuracy: 81.648570 %\n",
            "Epoch 00050 | Loss 0.3698 | Time(s) 5.8200 | Accuracy: 81.740327 %\n",
            "Epoch 00051 | Loss 0.3614 | Time(s) 5.8208 | Accuracy: 81.643472 %\n",
            "Epoch 00052 | Loss 0.3631 | Time(s) 5.8211 | Accuracy: 81.750523 %\n",
            "Epoch 00053 | Loss 0.3424 | Time(s) 5.8213 | Accuracy: 81.876689 %\n",
            "Epoch 00054 | Loss 0.3667 | Time(s) 5.8194 | Accuracy: 81.954427 %\n",
            "Epoch 00055 | Loss 0.3949 | Time(s) 5.8184 | Accuracy: 81.897079 %\n",
            "Epoch 00056 | Loss 0.3321 | Time(s) 5.8178 | Accuracy: 81.776011 %\n",
            "Epoch 00057 | Loss 0.3460 | Time(s) 5.8164 | Accuracy: 81.638375 %\n",
            "Epoch 00058 | Loss 0.3461 | Time(s) 5.8170 | Accuracy: 81.609064 %\n",
            "Epoch 00059 | Loss 0.4151 | Time(s) 5.8171 | Accuracy: 81.719937 %\n",
            "Epoch 00060 | Loss 0.3898 | Time(s) 5.8186 | Accuracy: 81.614161 %\n",
            "Epoch 00061 | Loss 0.3781 | Time(s) 5.8178 | Accuracy: 81.696998 %\n",
            "Epoch 00062 | Loss 0.3309 | Time(s) 5.8206 | Accuracy: 81.959525 %\n",
            "Epoch 00063 | Loss 0.3182 | Time(s) 5.8204 | Accuracy: 82.127746 %\n",
            "Epoch 00064 | Loss 0.3291 | Time(s) 5.8209 | Accuracy: 82.072947 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0509 | Time(s) 5.8732 | Accuracy: 42.602080 %\n",
            "Epoch 00001 | Loss 1.0479 | Time(s) 5.8101 | Accuracy: 48.519142 %\n",
            "Epoch 00002 | Loss 1.0210 | Time(s) 5.8140 | Accuracy: 48.610899 %\n",
            "Epoch 00003 | Loss 0.9223 | Time(s) 5.8215 | Accuracy: 51.234898 %\n",
            "Epoch 00004 | Loss 0.8995 | Time(s) 5.8074 | Accuracy: 52.421369 %\n",
            "Epoch 00005 | Loss 0.9145 | Time(s) 5.7946 | Accuracy: 53.183463 %\n",
            "Epoch 00006 | Loss 0.8881 | Time(s) 5.7964 | Accuracy: 53.312178 %\n",
            "Epoch 00007 | Loss 0.9371 | Time(s) 5.7940 | Accuracy: 53.112097 %\n",
            "Epoch 00008 | Loss 0.8350 | Time(s) 5.8073 | Accuracy: 53.245909 %\n",
            "Epoch 00009 | Loss 0.8339 | Time(s) 5.8067 | Accuracy: 53.282867 %\n",
            "Epoch 00010 | Loss 0.9012 | Time(s) 5.8024 | Accuracy: 53.291788 %\n",
            "Epoch 00011 | Loss 0.8176 | Time(s) 5.8003 | Accuracy: 53.011419 %\n",
            "Epoch 00012 | Loss 0.8864 | Time(s) 5.8017 | Accuracy: 53.129938 %\n",
            "Epoch 00013 | Loss 0.8514 | Time(s) 5.8037 | Accuracy: 53.361880 %\n",
            "Epoch 00014 | Loss 0.8436 | Time(s) 5.8033 | Accuracy: 52.965540 %\n",
            "Epoch 00015 | Loss 0.8794 | Time(s) 5.8060 | Accuracy: 52.270989 %\n",
            "Epoch 00016 | Loss 0.8417 | Time(s) 5.7998 | Accuracy: 52.810063 %\n",
            "Epoch 00017 | Loss 0.9105 | Time(s) 5.7988 | Accuracy: 53.048376 %\n",
            "Epoch 00018 | Loss 0.8404 | Time(s) 5.7950 | Accuracy: 53.011419 %\n",
            "Epoch 00019 | Loss 0.7656 | Time(s) 5.7901 | Accuracy: 52.050517 %\n",
            "Epoch 00020 | Loss 0.8499 | Time(s) 5.7893 | Accuracy: 51.665647 %\n",
            "Epoch 00021 | Loss 0.8259 | Time(s) 5.7831 | Accuracy: 52.290106 %\n",
            "Epoch 00022 | Loss 0.8147 | Time(s) 5.7817 | Accuracy: 52.732324 %\n",
            "Epoch 00023 | Loss 0.8055 | Time(s) 5.7804 | Accuracy: 52.962991 %\n",
            "Epoch 00024 | Loss 0.8068 | Time(s) 5.7828 | Accuracy: 52.966814 %\n",
            "Epoch 00025 | Loss 0.7408 | Time(s) 5.7822 | Accuracy: 52.830453 %\n",
            "Epoch 00026 | Loss 0.8116 | Time(s) 5.7818 | Accuracy: 52.854667 %\n",
            "Epoch 00027 | Loss 0.8029 | Time(s) 5.7806 | Accuracy: 52.718306 %\n",
            "Epoch 00028 | Loss 0.7987 | Time(s) 5.7810 | Accuracy: 52.731050 %\n",
            "Epoch 00029 | Loss 0.7968 | Time(s) 5.7813 | Accuracy: 52.878881 %\n",
            "Epoch 00030 | Loss 0.7245 | Time(s) 5.7795 | Accuracy: 52.994851 %\n",
            "Epoch 00031 | Loss 0.7232 | Time(s) 5.7772 | Accuracy: 53.042004 %\n",
            "Epoch 00032 | Loss 0.7269 | Time(s) 5.7761 | Accuracy: 52.886527 %\n",
            "Epoch 00033 | Loss 0.7230 | Time(s) 5.7768 | Accuracy: 52.539889 %\n",
            "Epoch 00034 | Loss 0.7829 | Time(s) 5.7757 | Accuracy: 52.616353 %\n",
            "Epoch 00035 | Loss 0.7018 | Time(s) 5.7764 | Accuracy: 52.639292 %\n",
            "Epoch 00036 | Loss 0.8016 | Time(s) 5.7775 | Accuracy: 52.697915 %\n",
            "Epoch 00037 | Loss 0.7620 | Time(s) 5.7765 | Accuracy: 52.708110 %\n",
            "Epoch 00038 | Loss 0.6868 | Time(s) 5.7787 | Accuracy: 52.732324 %\n",
            "Epoch 00039 | Loss 0.7811 | Time(s) 5.7774 | Accuracy: 52.477443 %\n",
            "Epoch 00040 | Loss 0.7422 | Time(s) 5.7771 | Accuracy: 52.514401 %\n",
            "Epoch 00041 | Loss 0.6818 | Time(s) 5.7777 | Accuracy: 52.778203 %\n",
            "Epoch 00042 | Loss 0.7620 | Time(s) 5.7769 | Accuracy: 52.797319 %\n",
            "Epoch 00043 | Loss 0.6646 | Time(s) 5.7764 | Accuracy: 52.394607 %\n",
            "Epoch 00044 | Loss 0.7699 | Time(s) 5.7772 | Accuracy: 52.441760 %\n",
            "Epoch 00045 | Loss 0.7741 | Time(s) 5.7770 | Accuracy: 52.734873 %\n",
            "Epoch 00046 | Loss 0.6618 | Time(s) 5.7764 | Accuracy: 52.848295 %\n",
            "Epoch 00047 | Loss 0.6652 | Time(s) 5.7755 | Accuracy: 52.660957 %\n",
            "Epoch 00048 | Loss 0.7686 | Time(s) 5.7731 | Accuracy: 52.309222 %\n",
            "Epoch 00049 | Loss 0.6981 | Time(s) 5.7737 | Accuracy: 52.315594 %\n",
            "Epoch 00050 | Loss 0.7483 | Time(s) 5.7740 | Accuracy: 52.532242 %\n",
            "Epoch 00051 | Loss 0.7425 | Time(s) 5.7745 | Accuracy: 52.501657 %\n",
            "Epoch 00052 | Loss 0.6485 | Time(s) 5.7738 | Accuracy: 52.427741 %\n",
            "Epoch 00053 | Loss 0.7277 | Time(s) 5.7756 | Accuracy: 52.403528 %\n",
            "Epoch 00054 | Loss 0.7252 | Time(s) 5.7760 | Accuracy: 52.569200 %\n",
            "Epoch 00055 | Loss 0.7315 | Time(s) 5.7762 | Accuracy: 52.346179 %\n",
            "Epoch 00056 | Loss 0.7023 | Time(s) 5.7764 | Accuracy: 52.125707 %\n",
            "Epoch 00057 | Loss 0.6760 | Time(s) 5.7763 | Accuracy: 52.263343 %\n",
            "Epoch 00058 | Loss 0.6445 | Time(s) 5.7783 | Accuracy: 52.367844 %\n",
            "Epoch 00059 | Loss 0.7212 | Time(s) 5.7771 | Accuracy: 52.311770 %\n",
            "Epoch 00060 | Loss 0.7540 | Time(s) 5.7775 | Accuracy: 52.310496 %\n",
            "Epoch 00061 | Loss 0.7213 | Time(s) 5.7792 | Accuracy: 52.465973 %\n",
            "Epoch 00062 | Loss 0.6217 | Time(s) 5.7801 | Accuracy: 52.557731 %\n",
            "Epoch 00063 | Loss 0.6940 | Time(s) 5.7824 | Accuracy: 52.262069 %\n",
            "Epoch 00064 | Loss 0.7186 | Time(s) 5.7825 | Accuracy: 52.017383 %\n",
            "Results stored.\n",
            "Starting new training\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Epoch 00000 | Loss 1.0555 | Time(s) 5.7880 | Accuracy: 31.386043 %\n",
            "Epoch 00001 | Loss 1.0256 | Time(s) 5.8111 | Accuracy: 45.394301 %\n",
            "Epoch 00002 | Loss 0.9902 | Time(s) 5.8165 | Accuracy: 51.640159 %\n",
            "Epoch 00003 | Loss 0.9269 | Time(s) 5.7842 | Accuracy: 53.900953 %\n",
            "Epoch 00004 | Loss 1.0293 | Time(s) 5.7944 | Accuracy: 54.752256 %\n",
            "Epoch 00005 | Loss 0.9683 | Time(s) 5.8029 | Accuracy: 54.845287 %\n",
            "Epoch 00006 | Loss 0.9454 | Time(s) 5.8121 | Accuracy: 56.319774 %\n",
            "Epoch 00007 | Loss 0.8757 | Time(s) 5.8069 | Accuracy: 73.089667 %\n",
            "Epoch 00008 | Loss 0.7679 | Time(s) 5.8153 | Accuracy: 79.590406 %\n",
            "Epoch 00009 | Loss 0.7678 | Time(s) 5.8176 | Accuracy: 79.529235 %\n",
            "Epoch 00010 | Loss 0.6947 | Time(s) 5.8141 | Accuracy: 79.326604 %\n",
            "Epoch 00011 | Loss 0.6800 | Time(s) 5.8154 | Accuracy: 78.425600 %\n",
            "Epoch 00012 | Loss 0.6389 | Time(s) 5.8161 | Accuracy: 79.517765 %\n",
            "Epoch 00013 | Loss 0.6239 | Time(s) 5.8132 | Accuracy: 80.100168 %\n",
            "Epoch 00014 | Loss 0.6082 | Time(s) 5.8171 | Accuracy: 79.974002 %\n",
            "Epoch 00015 | Loss 0.6442 | Time(s) 5.8149 | Accuracy: 80.948922 %\n",
            "Epoch 00016 | Loss 0.5770 | Time(s) 5.8201 | Accuracy: 81.364378 %\n",
            "Epoch 00017 | Loss 0.6402 | Time(s) 5.8172 | Accuracy: 81.231840 %\n",
            "Epoch 00018 | Loss 0.5225 | Time(s) 5.8167 | Accuracy: 81.384768 %\n",
            "Epoch 00019 | Loss 0.5400 | Time(s) 5.8154 | Accuracy: 81.182138 %\n",
            "Epoch 00020 | Loss 0.5365 | Time(s) 5.8208 | Accuracy: 80.704236 %\n",
            "Epoch 00021 | Loss 0.5285 | Time(s) 5.8219 | Accuracy: 80.320640 %\n",
            "Epoch 00022 | Loss 0.4992 | Time(s) 5.8218 | Accuracy: 81.286639 %\n",
            "Epoch 00023 | Loss 0.5117 | Time(s) 5.8235 | Accuracy: 81.818066 %\n",
            "Epoch 00024 | Loss 0.6080 | Time(s) 5.8214 | Accuracy: 81.708467 %\n",
            "Epoch 00025 | Loss 0.5031 | Time(s) 5.8221 | Accuracy: 81.420452 %\n",
            "Epoch 00026 | Loss 0.5450 | Time(s) 5.8222 | Accuracy: 81.410256 %\n",
            "Epoch 00027 | Loss 0.4546 | Time(s) 5.8229 | Accuracy: 81.524953 %\n",
            "Epoch 00028 | Loss 0.4471 | Time(s) 5.8241 | Accuracy: 81.592496 %\n",
            "Epoch 00029 | Loss 0.4744 | Time(s) 5.8228 | Accuracy: 81.765815 %\n",
            "Epoch 00030 | Loss 0.4265 | Time(s) 5.8245 | Accuracy: 81.444665 %\n",
            "Epoch 00031 | Loss 0.4125 | Time(s) 5.8240 | Accuracy: 81.221644 %\n",
            "Epoch 00032 | Loss 0.3945 | Time(s) 5.8234 | Accuracy: 80.700413 %\n",
            "Epoch 00033 | Loss 0.4954 | Time(s) 5.8233 | Accuracy: 80.583168 %\n",
            "Epoch 00034 | Loss 0.4495 | Time(s) 5.8256 | Accuracy: 81.723760 %\n",
            "Epoch 00035 | Loss 0.4819 | Time(s) 5.8257 | Accuracy: 81.955702 %\n",
            "Epoch 00036 | Loss 0.4059 | Time(s) 5.8262 | Accuracy: 82.053831 %\n",
            "Epoch 00037 | Loss 0.4841 | Time(s) 5.8250 | Accuracy: 82.007952 %\n",
            "Epoch 00038 | Loss 0.4731 | Time(s) 5.8254 | Accuracy: 81.946781 %\n",
            "Epoch 00039 | Loss 0.4160 | Time(s) 5.8251 | Accuracy: 81.909823 %\n",
            "Epoch 00040 | Loss 0.3931 | Time(s) 5.8270 | Accuracy: 81.806596 %\n",
            "Epoch 00041 | Loss 0.4455 | Time(s) 5.8267 | Accuracy: 81.758169 %\n",
            "Epoch 00042 | Loss 0.3885 | Time(s) 5.8266 | Accuracy: 81.690625 %\n",
            "Epoch 00043 | Loss 0.4083 | Time(s) 5.8276 | Accuracy: 81.626905 %\n",
            "Epoch 00044 | Loss 0.3760 | Time(s) 5.8264 | Accuracy: 81.688077 %\n",
            "Epoch 00045 | Loss 0.4339 | Time(s) 5.8278 | Accuracy: 81.865219 %\n",
            "Epoch 00046 | Loss 0.3714 | Time(s) 5.8262 | Accuracy: 81.852475 %\n",
            "Epoch 00047 | Loss 0.3404 | Time(s) 5.8242 | Accuracy: 81.383494 %\n",
            "Epoch 00048 | Loss 0.3352 | Time(s) 5.8202 | Accuracy: 81.055972 %\n",
            "Epoch 00049 | Loss 0.3695 | Time(s) 5.8152 | Accuracy: 81.500739 %\n",
            "Epoch 00050 | Loss 0.3746 | Time(s) 5.8101 | Accuracy: 82.081868 %\n",
            "Epoch 00051 | Loss 0.4058 | Time(s) 5.8073 | Accuracy: 82.060203 %\n",
            "Epoch 00052 | Loss 0.3723 | Time(s) 5.8050 | Accuracy: 81.671509 %\n",
            "Epoch 00053 | Loss 0.4665 | Time(s) 5.8017 | Accuracy: 81.587399 %\n",
            "Epoch 00054 | Loss 0.3950 | Time(s) 5.7996 | Accuracy: 82.070398 %\n",
            "Epoch 00055 | Loss 0.4020 | Time(s) 5.7981 | Accuracy: 81.874140 %\n",
            "Epoch 00056 | Loss 0.4175 | Time(s) 5.7951 | Accuracy: 81.886884 %\n",
            "Epoch 00057 | Loss 0.4004 | Time(s) 5.7927 | Accuracy: 81.779834 %\n",
            "Epoch 00058 | Loss 0.3391 | Time(s) 5.7893 | Accuracy: 81.628180 %\n",
            "Epoch 00059 | Loss 0.3442 | Time(s) 5.7866 | Accuracy: 81.616710 %\n",
            "Epoch 00060 | Loss 0.3534 | Time(s) 5.7831 | Accuracy: 81.600143 %\n",
            "Epoch 00061 | Loss 0.4021 | Time(s) 5.7812 | Accuracy: 81.508386 %\n",
            "Epoch 00062 | Loss 0.4143 | Time(s) 5.7791 | Accuracy: 81.238212 %\n",
            "Epoch 00063 | Loss 0.3721 | Time(s) 5.7778 | Accuracy: 81.480349 %\n",
            "Epoch 00064 | Loss 0.3439 | Time(s) 5.7756 | Accuracy: 81.758169 %\n",
            "Results stored.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-ipAEZcN5LT",
        "colab_type": "text"
      },
      "source": [
        "#On graphic"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUjMKV_LXIu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accPointsPubMed = [0]*n_of_epochs\n",
        "lossPointsPubMed = [0]*n_of_epochs\n",
        "\n",
        "for d in range(n_of_training_cycles):\n",
        "  for i in range(n_of_epochs):\n",
        "    accPointsPubMed[i] = accPointsPubMed[i] + averageAccPubMed[d][i]\n",
        "    lossPointsPubMed[i] = lossPointsPubMed[i] + averageLossPubMed[d][i]\n",
        "\n",
        "for i in range(n_of_epochs):\n",
        "  accPointsPubMed[i] = accPointsPubMed[i]/n_of_training_cycles\n",
        "  lossPointsPubMed[i] = lossPointsPubMed[i]/n_of_training_cycles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80Erkh6MqGjl",
        "colab_type": "code",
        "outputId": "25969ecc-2210-4d1b-d054-4bf13dc973bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "maxAccPubMed = np.argmax(accPointsPubMed)\n",
        "maxAccPubMed"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1TBb1GlN7rC",
        "colab_type": "code",
        "outputId": "0b667eea-b252-4ab9-d8ee-105d99af7426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "axisX = range(n_of_epochs)\n",
        "\n",
        "plt.plot(axisX, accPointsPubMed, color='orange', label='PubMed')\n",
        "#plt.plot(axisX, pointsPubMed, color='blue', label='PubMed')\n",
        "plt.plot([maxAccPubMed], accPointsPubMed[maxAccPubMed], marker='o', color='red')\n",
        "#plt.plot([maxAccPubMed], pointsPubMed[maxAccPubMed], marker='o', color='blue')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXEQkoRTgIiIJ9f8Bn/cx9DDr9tkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5mBRqr82F5e",
        "colab_type": "code",
        "outputId": "4bcb4ed2-04fd-4564-87b0-c1739b41de08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Max accuracy PubMed at {:03d} epochs with value {:05f}\"\n",
        "  .format(maxAccPubMed, pointsPubMed[maxAccPubMed]))\n",
        "#print(\"Max accuracy PubMed at {:03d} epochs with value {:05f}\"\n",
        "#  .format(maxAccPubMed, pointsPubMed[maxAccPubMed]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max accuracy Cora at 035 epochs with value 81.955702\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdTRUep-4FFJ",
        "colab_type": "code",
        "outputId": "4a308258-4a36-4867-e90d-f6ea6704be3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "plt.plot(axisX, lossPointsPubMed, color='orange', label='PubMed')\n",
        "#plt.plot(axisX, pointsLossPubMed, color='blue', label='PubMed')\n",
        "plt.plot([maxAccPubMed], lossPointsPubMed[maxAccPubMed], marker='o', color='red')\n",
        "#plt.plot([maxAccPubMed], pointsLossPubMed[maxAccPubMed].item(), marker='o',color='blue')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD5CAYAAAA3Os7hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEsT/B7qY1l2UBdruAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KUKoacJ2t6h",
        "colab_type": "code",
        "outputId": "3f41a176-2651-4c3c-9943-423aa50b9d4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"For PubMed, at epoch {:03d}, loss was {:05f}\"\n",
        "  .format(maxAccPubMed, lossPointsPubMed[maxAccPubMed]))\n",
        "#print(\"For PubMed, at epoch {:03d}, loss was {:05f}\"\n",
        "#  .format(maxAccPubMed, pointsLossPubMed[maxAccPubMed]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For Cora, at epoch 035, loss was 0.529607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5EwZ8SD9nSW",
        "colab_type": "text"
      },
      "source": [
        "#Our algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eizH41lSch10",
        "colab_type": "text"
      },
      "source": [
        "For our algorithm we need to store somewhere some information. In particular for each training cycle we will update the historical activation in order to bring an efficent approximation. Also, we need to store the last recent h_0 and h_1, for computational purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMnnSp8_LU8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HMatrices():\n",
        "  def __init__(self):\n",
        "    self.historical_activation_1 = th.Tensor(np.zeros((19717,500)))\n",
        "    self.historical_activation_2 = th.Tensor(np.zeros((19717,16)))\n",
        "    self.h_0 = th.Tensor([])\n",
        "    self.h_1 = th.Tensor([])\n",
        "\n",
        "  def updateHMatrix(self,features,x):\n",
        "    self.h_0 = features\n",
        "    self.h_1 = x\n",
        "  \n",
        "  def updateActivation(self, row, level, a):\n",
        "    if level == 0:\n",
        "      self.historical_activation_1[row] = a\n",
        "    elif level == 1:\n",
        "      self.historical_activation_2[row] = a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0rSVV_I1S-03",
        "colab": {}
      },
      "source": [
        "class SimpleNodeApplyModule(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, activation):\n",
        "        super(SimpleNodeApplyModule, self).__init__()\n",
        "        self.linear = nn.Linear(in_feats, out_feats)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self, node):\n",
        "        #type of node: dgl.udf.NodeBatch\n",
        "        #type of node.data['h']: torch.Tensor\n",
        "        z = self.linear(node.data['h'])\n",
        "        h = self.activation(z)\n",
        "        return {'h' : h}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fXAYnjwgTCfR",
        "colab": {}
      },
      "source": [
        "class SimpleGCN(nn.Module):\n",
        "    def __init__(self, in_feats, out_feats, activation):\n",
        "        super(SimpleGCN, self).__init__()\n",
        "        self.apply_mod = SimpleNodeApplyModule(in_feats, out_feats, activation)\n",
        "\n",
        "    def forward(self, g, feature):\n",
        "        g.ndata['h'] = feature\n",
        "        g.update_all(gcn_msg, gcn_reduce)\n",
        "        g.apply_nodes(func=self.apply_mod)\n",
        "        return g.ndata.pop('h')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oBqQpHvLTHUD",
        "outputId": "3e2d3169-6e92-44b2-d98a-f455be374ab3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.gcn1 = SimpleGCN(500, 16, F.relu)\n",
        "        self.gcn2 = SimpleGCN(16, 3, F.relu)\n",
        "\n",
        "    def forward(self, g, features, propMatrix, historicalActivation, hContainer):\n",
        "        #type of 'g': dgl.graph.DGLGraph\n",
        "        x = self.gcn1(g, features)\n",
        "        hContainer.updateHMatrix(features, x)\n",
        "        # CV = (P'_l*(H_l-H'_l)+P*H'_l)\n",
        "        x = th.mm(propMatrix,x.sub(historicalActivation)).add(th.mm(P,historicalActivation))\n",
        "        x = self.gcn2(g, x)\n",
        "        return x\n",
        "\n",
        "simpleNet = SimpleNet()\n",
        "print(simpleNet)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SimpleNet(\n",
            "  (gcn1): SimpleGCN(\n",
            "    (apply_mod): SimpleNodeApplyModule(\n",
            "      (linear): Linear(in_features=500, out_features=16, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (gcn2): SimpleGCN(\n",
            "    (apply_mod): SimpleNodeApplyModule(\n",
            "      (linear): Linear(in_features=16, out_features=3, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "omo3JIZQT8Br",
        "outputId": "5bfc971c-94e0-416c-83e0-878b24c0f731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import time\n",
        "import collections\n",
        "import numpy as np\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#init counters\n",
        "averageAccPubMed_our = []\n",
        "averageLossPubMed_our = []\n",
        "#\n",
        "n_of_training_cycles = 1\n",
        "#\n",
        "for t in range(n_of_training_cycles):\n",
        "  print(\"Starting new training cycle\")\n",
        "\n",
        "  simpleNet = SimpleNet()\n",
        "  #point to show on graph\n",
        "  pointsOurPubMed=dict()\n",
        "  pointsOurLossPubMed=dict()\n",
        "\n",
        "  with th.no_grad():\n",
        "    #get data\n",
        "    g, features, labels, mask = load_pubmed_data()\n",
        "\n",
        "    print(g)\n",
        "    n_masks_to_try = 4\n",
        "    ourOptimizer = th.optim.Adam(simpleNet.parameters(), lr=2e-3)\n",
        "    ourOptimizer.state = collections.defaultdict(dict)\n",
        "    dur = []\n",
        "\n",
        "    #running algOne for each mask\n",
        "    t0 = time.time()\n",
        "    print(\"Computing algorithm 1 for each mask...\")\n",
        "    resForMasks = dict()\n",
        "    for m in masks:\n",
        "      resForMasks[m] = algOne(m)\n",
        "    print(\"{:03f} seconds\".format(time.time()-t0))\n",
        "\n",
        "  #this is an object storing some constant data\n",
        "  hs = HMatrices()\n",
        "\n",
        "  for epoch in range(n_of_epochs):\n",
        "        \n",
        "      t0 = time.time()\n",
        "      #getting only some masks (4)\n",
        "      masksToTry = random.sample(masks,n_masks_to_try)\n",
        "      for m in masksToTry:\n",
        "          #calling 'net(...)' it asks to the GCN to compute the forward\n",
        "\n",
        "          #M=(P'_l*(H_l-H'_l)+P*H'_l)\n",
        "          #H_l = feature\n",
        "          #P': progationMatrix\n",
        "          #H': get from initialized matrix\n",
        "          #P: computed before\n",
        "          CV = th.mm(resForMasks[m][1][0].to_dense(),features.sub(hs.historical_activation_1.detach())).add(th.mm(P,hs.historical_activation_1.detach()))\n",
        "          logits = simpleNet(g, CV, resForMasks[m][1][1].to_dense(), hs.historical_activation_2.detach(), hs)\n",
        "          logp = F.log_softmax(logits, 1)\n",
        "\n",
        "          #compute loss like the negative log likelihood loss\n",
        "          ourLoss = F.nll_loss(logp[m], labels[m])\n",
        "\n",
        "          #Since the backward() function accumulates gradients, and you \n",
        "          #don’t want to mix up gradients between minibatch, you have \n",
        "          #to zero them out at the start of a new minibatch. This is \n",
        "          #exactly like how a general (additive) accumulator variable is \n",
        "          #initialized to 0 in code.\n",
        "          ourOptimizer.zero_grad()\n",
        "\n",
        "          #update network weights by loss\n",
        "          #ourLoss.backward(retain_graph=True)\n",
        "          ourLoss.backward()\n",
        "\n",
        "          #update optimizer's values after backward\n",
        "          ourOptimizer.step()\n",
        "\n",
        "          #computing accuracy\n",
        "          i = 0\n",
        "          matched = 0\n",
        "          while i < 19717:\n",
        "            if m[i] == 0:\n",
        "              #getting index of the maximum\n",
        "              j = 0\n",
        "              max = None\n",
        "              jMax = 0\n",
        "              for a in logp[i]:\n",
        "                if max==None:\n",
        "                  max = a.item()\n",
        "                  jMax = j\n",
        "                elif max < a.item():\n",
        "                  max = a.item()\n",
        "                  jMax = j\n",
        "                j = j + 1\n",
        "              if jMax == labels[i]:\n",
        "                matched = matched + 1\n",
        "            i = i + 1\n",
        "          acc = matched/(19717-size_masks)*100\n",
        "\n",
        "          if epoch not in pointsOurPubMed:\n",
        "            pointsOurPubMed[epoch] = 0\n",
        "            pointsOurLossPubMed[epoch] = 0\n",
        "          pointsOurPubMed[epoch] = pointsOurPubMed[epoch] + acc\n",
        "          pointsOurLossPubMed[epoch] = pointsOurLossPubMed[epoch] + ourLoss.item()\n",
        "          \n",
        "          #update historical activation\n",
        "          for i in range(2):\n",
        "            recField = resForMasks[m][0][i]\n",
        "            k = 0\n",
        "            if i == 0:\n",
        "              for n in recField:\n",
        "                if n == 1:\n",
        "                  hs.updateActivation(k,i,hs.h_0[k])\n",
        "                k = k + 1\n",
        "            elif i == 1:\n",
        "              for n in recField:\n",
        "                if n == 1:\n",
        "                  hs.updateActivation(k,i,hs.h_1[k])\n",
        "                k = k + 1\n",
        "\n",
        "      dur.append(time.time() - t0)\n",
        "      \n",
        "      pointsOurPubMed[epoch] = pointsOurPubMed[epoch]/n_masks_to_try\n",
        "      pointsOurLossPubMed[epoch] = pointsOurLossPubMed[epoch]/n_masks_to_try\n",
        "\n",
        "      print(\"Epoch {:05d} | Loss {:.4f} | Time(s) {:.4f} | Accuracy: {:.6f}\".format(\n",
        "              epoch, ourLoss.item(), np.mean(dur), pointsOurPubMed[epoch]))\n",
        "  \n",
        "  averageAccPubMed_our.append(pointsOurPubMed)\n",
        "  averageLossPubMed_our.append(pointsOurLossPubMed)\n",
        "  print(\"Results stored\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting new training cycle\n",
            "Finished data loading and preprocessing.\n",
            "  NumNodes: 19717\n",
            "  NumEdges: 88651\n",
            "  NumFeats: 500\n",
            "  NumClasses: 3\n",
            "  NumTrainingSamples: 60\n",
            "  NumValidationSamples: 500\n",
            "  NumTestSamples: 1000\n",
            "\n",
            "DGLGraph(num_nodes=19717, num_edges=108365,\n",
            "         ndata_schemes={}\n",
            "         edata_schemes={})\n",
            "Computing algorithm 1 for each mask...\n",
            "23.862541 seconds\n",
            "Epoch 00000 | Loss 1.1034 | Time(s) 39.9669 | Accuracy: 20.807208\n",
            "Epoch 00001 | Loss 1.0968 | Time(s) 39.7660 | Accuracy: 21.003466\n",
            "Epoch 00002 | Loss 1.0937 | Time(s) 39.7898 | Accuracy: 21.924861\n",
            "Epoch 00003 | Loss 1.0804 | Time(s) 39.7343 | Accuracy: 23.731967\n",
            "Epoch 00004 | Loss 1.0723 | Time(s) 39.6922 | Accuracy: 27.588316\n",
            "Epoch 00005 | Loss 1.0602 | Time(s) 39.6738 | Accuracy: 33.832900\n",
            "Epoch 00006 | Loss 1.0180 | Time(s) 39.6227 | Accuracy: 39.084722\n",
            "Epoch 00007 | Loss 0.9964 | Time(s) 39.6646 | Accuracy: 44.082938\n",
            "Epoch 00008 | Loss 1.0361 | Time(s) 39.6958 | Accuracy: 49.049294\n",
            "Epoch 00009 | Loss 0.9179 | Time(s) 39.7637 | Accuracy: 53.852526\n",
            "Epoch 00010 | Loss 0.9357 | Time(s) 39.8202 | Accuracy: 58.158740\n",
            "Epoch 00011 | Loss 0.9411 | Time(s) 40.0672 | Accuracy: 61.167610\n",
            "Epoch 00012 | Loss 0.8780 | Time(s) 39.9921 | Accuracy: 63.469185\n",
            "Epoch 00013 | Loss 0.8521 | Time(s) 39.9214 | Accuracy: 64.987001\n",
            "Epoch 00014 | Loss 0.8181 | Time(s) 39.8980 | Accuracy: 66.265229\n",
            "Epoch 00015 | Loss 0.7968 | Time(s) 39.8960 | Accuracy: 67.029872\n",
            "Epoch 00016 | Loss 0.7445 | Time(s) 39.8420 | Accuracy: 67.978029\n",
            "Epoch 00017 | Loss 0.7701 | Time(s) 39.8040 | Accuracy: 68.927461\n",
            "Epoch 00018 | Loss 0.7543 | Time(s) 39.7615 | Accuracy: 70.004333\n",
            "Epoch 00019 | Loss 0.6997 | Time(s) 39.7623 | Accuracy: 71.885355\n",
            "Epoch 00020 | Loss 0.6964 | Time(s) 39.7287 | Accuracy: 72.706071\n",
            "Epoch 00021 | Loss 0.7407 | Time(s) 39.7383 | Accuracy: 73.303767\n",
            "Epoch 00022 | Loss 0.6851 | Time(s) 39.7418 | Accuracy: 73.967732\n",
            "Epoch 00023 | Loss 0.7184 | Time(s) 39.7715 | Accuracy: 73.942244\n",
            "Epoch 00024 | Loss 0.7145 | Time(s) 39.7775 | Accuracy: 74.265943\n",
            "Epoch 00025 | Loss 0.6972 | Time(s) 39.7797 | Accuracy: 74.522098\n",
            "Epoch 00026 | Loss 0.6936 | Time(s) 39.7946 | Accuracy: 73.974104\n",
            "Epoch 00027 | Loss 0.5877 | Time(s) 39.8288 | Accuracy: 74.406127\n",
            "Epoch 00028 | Loss 0.7201 | Time(s) 39.8129 | Accuracy: 75.112148\n",
            "Epoch 00029 | Loss 0.6238 | Time(s) 39.8181 | Accuracy: 75.173319\n",
            "Epoch 00030 | Loss 0.5384 | Time(s) 39.8165 | Accuracy: 75.273997\n",
            "Epoch 00031 | Loss 0.6168 | Time(s) 39.8445 | Accuracy: 75.160575\n",
            "Epoch 00032 | Loss 0.5045 | Time(s) 39.8568 | Accuracy: 75.314778\n",
            "Epoch 00033 | Loss 0.6417 | Time(s) 39.8849 | Accuracy: 75.177142\n",
            "Epoch 00034 | Loss 0.6087 | Time(s) 39.9074 | Accuracy: 75.030586\n",
            "Epoch 00035 | Loss 0.6114 | Time(s) 39.9452 | Accuracy: 75.100678\n",
            "Epoch 00036 | Loss 0.5790 | Time(s) 39.9625 | Accuracy: 74.821583\n",
            "Epoch 00037 | Loss 0.6175 | Time(s) 39.9898 | Accuracy: 74.295254\n",
            "Epoch 00038 | Loss 0.6020 | Time(s) 40.0089 | Accuracy: 73.785492\n",
            "Epoch 00039 | Loss 0.6251 | Time(s) 40.0055 | Accuracy: 74.415048\n",
            "Epoch 00040 | Loss 0.5547 | Time(s) 39.9901 | Accuracy: 74.571800\n",
            "Epoch 00041 | Loss 0.4580 | Time(s) 39.9883 | Accuracy: 74.236631\n",
            "Epoch 00042 | Loss 0.5898 | Time(s) 40.0067 | Accuracy: 74.402304\n",
            "Epoch 00043 | Loss 0.5546 | Time(s) 40.0024 | Accuracy: 73.526788\n",
            "Epoch 00044 | Loss 0.5912 | Time(s) 40.0039 | Accuracy: 72.865372\n",
            "Epoch 00045 | Loss 0.5549 | Time(s) 40.0128 | Accuracy: 72.609216\n",
            "Epoch 00046 | Loss 0.7022 | Time(s) 40.0063 | Accuracy: 73.839017\n",
            "Epoch 00047 | Loss 0.5135 | Time(s) 39.9981 | Accuracy: 73.635112\n",
            "Epoch 00048 | Loss 0.9271 | Time(s) 39.9819 | Accuracy: 73.751083\n",
            "Epoch 00049 | Loss 0.5430 | Time(s) 39.9763 | Accuracy: 73.547178\n",
            "Epoch 00050 | Loss 0.6750 | Time(s) 39.9631 | Accuracy: 73.293572\n",
            "Epoch 00051 | Loss 0.6063 | Time(s) 39.9464 | Accuracy: 73.172503\n",
            "Epoch 00052 | Loss 0.4997 | Time(s) 39.9325 | Accuracy: 73.138095\n",
            "Epoch 00053 | Loss 0.7441 | Time(s) 39.9172 | Accuracy: 72.907427\n",
            "Epoch 00054 | Loss 0.4026 | Time(s) 39.8926 | Accuracy: 72.717541\n",
            "Epoch 00055 | Loss 0.6397 | Time(s) 39.8796 | Accuracy: 72.384921\n",
            "Epoch 00056 | Loss 0.5077 | Time(s) 39.8652 | Accuracy: 71.880257\n",
            "Epoch 00057 | Loss 1.2664 | Time(s) 39.8518 | Accuracy: 73.076923\n",
            "Epoch 00058 | Loss 0.4221 | Time(s) 39.8390 | Accuracy: 72.856451\n",
            "Epoch 00059 | Loss 0.6561 | Time(s) 39.8204 | Accuracy: 71.755365\n",
            "Epoch 00060 | Loss 0.4873 | Time(s) 39.8038 | Accuracy: 70.682316\n",
            "Epoch 00061 | Loss 0.5686 | Time(s) 39.7951 | Accuracy: 71.802518\n",
            "Epoch 00062 | Loss 0.4827 | Time(s) 39.7848 | Accuracy: 71.160218\n",
            "Epoch 00063 | Loss 0.7836 | Time(s) 39.7724 | Accuracy: 69.471632\n",
            "Epoch 00064 | Loss 0.5695 | Time(s) 39.7729 | Accuracy: 67.840394\n",
            "Results stored\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjvSIfKTdIgA",
        "colab_type": "text"
      },
      "source": [
        "#On graphic (our algorithm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HJAqZbH4Makd",
        "colab": {}
      },
      "source": [
        "accPointsPubMed_our = [0]*n_of_epochs\n",
        "lossPointsPubMed_our = [0]*n_of_epochs\n",
        "\n",
        "for d in range(n_of_training_cycles):\n",
        "  for i in range(n_of_epochs):\n",
        "    accPointsPubMed_our[i] = accPointsPubMed_our[i] + averageAccPubMed_our[d][i]\n",
        "    lossPointsPubMed_our[i] = lossPointsPubMed_our[i] + averageLossPubMed_our[d][i]\n",
        "\n",
        "for i in range(n_of_epochs):\n",
        "  accPointsPubMed_our[i] = accPointsPubMed_our[i]/n_of_training_cycles\n",
        "  lossPointsPubMed_our[i] = lossPointsPubMed_our[i]/n_of_training_cycles"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wkuOEgTMrDI",
        "colab_type": "code",
        "outputId": "9474a904-75c4-44cc-f75a-2283ca0cb855",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "maxAccPubMed_our = np.argmax(accPointsPubMed_our)\n",
        "maxAccPubMed_our"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLpUCy3YdMdo",
        "colab_type": "text"
      },
      "source": [
        "Accuracy of our algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-v9sNpqyQboC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "axisX = range(n_of_epochs)\n",
        "\n",
        "plt.plot(axisX, accPointsPubMed_our, color='orange', label='PubMed')\n",
        "#plt.plot(axisX, pointsPubMed, color='blue', label='PubMed')\n",
        "plt.plot([maxAccPubMed_our], accPointsPubMed_our[maxAccPubMed_our], marker='o', color='red')\n",
        "#plt.plot([maxAccPubMed], pointsPubMed[maxAccPubMed], marker='o', color='blue')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WaxZKGbNQpGw",
        "outputId": "38002bb7-49f9-41b9-fa87-714d7a3def00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Max accuracy PubMed at {:03d} epochs with value {:05f}\"\n",
        "  .format(maxAccPubMed_our, accPointsPubMed_our[maxAccPubMed_our]))\n",
        "#print(\"Max accuracy PubMed at {:03d} epochs with value {:05f}\"\n",
        "#  .format(maxAccPubMed, pointsPubMed[maxAccPubMed]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max accuracy Cora at 032 epochs with value 75.314778\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdFEyVh0dQow",
        "colab_type": "text"
      },
      "source": [
        "Loss values of our algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wCO9VsNgQ8bC",
        "colab": {}
      },
      "source": [
        "plt.plot(axisX, lossPointsPubMed_our, color='orange', label='PubMed')\n",
        "#plt.plot(axisX, pointsLossPubMed, color='blue', label='PubMed')\n",
        "plt.plot([maxAccPubMed_our], lossPointsPubMed_our[maxAccPubMed_our], marker='o', color='red')\n",
        "#plt.plot([maxAccPubMed], pointsLossPubMed[maxAccPubMed].item(), marker='o',color='blue')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wi2Qzd5YRGdg",
        "outputId": "acd105f6-b41c-4055-a7b8-45cefadb75b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"For PubMed, at epoch {:03d}, loss was {:05f}\"\n",
        "  .format(maxAccPubMed_our, lossPointsPubMed_our[maxAccPubMed_our]))\n",
        "#print(\"For PubMed, at epoch {:03d}, loss was {:05f}\"\n",
        "#  .format(maxAccPubMed, pointsLossPubMed[maxAccPubMed]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For Cora, at epoch 032, loss was 0.569470\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdOKWGe-SB94",
        "colab_type": "text"
      },
      "source": [
        "#Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jej5As7dUUs",
        "colab_type": "text"
      },
      "source": [
        "Comparison between accuracy of both algorithms. The red dots show the maximum values reached."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4jkhpiISEL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(axisX, accPointsPubMed, color='orange', label='PubMed')\n",
        "plt.plot(axisX, accPointsPubMed_our, color='blue', label='Our PubMed')\n",
        "plt.plot([maxAccPubMed], accPointsPubMed[maxAccPubMed], marker='o', color ='red')\n",
        "plt.plot([maxAccPubMed_our], accPointsPubMed_our[maxAccPubMed_our], marker='o', color='red')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX2zNuYpdfHI",
        "colab_type": "text"
      },
      "source": [
        "Comparison between the loss values of both algorithm. The red dots shows the loss values when the accuracy is at its maximum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WAxIo0lSv28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(axisX, lossPointsPubMed, color='orange', label='PubMed')\n",
        "plt.plot(axisX, lossPointsPubMed_our, color='blue', label='Our PubMed')\n",
        "plt.plot([maxAccPubMed], lossPointsPubMed[maxAccPubMed], marker='o', color ='red')\n",
        "plt.plot([maxAccPubMed_our], lossPointsPubMed_our[maxAccPubMed_our], marker='o', color='red')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}